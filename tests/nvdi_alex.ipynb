{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "501a9e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/_masked/__init__.py:223: UserWarning: Failed to initialize NumPy: module compiled against API version 0xf but this version of numpy is 0xd (Triggered internally at  /opt/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:68.)\n",
      "  example_input = torch.tensor([[-3, -2, -1], [0, 1, 2]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=12, stride=4, padding=0),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU())\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU())\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(9216, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.fc1(out)\n",
    "        out = F.adaptive_avg_pool2d(out.unsqueeze(1), output_size=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9ecf372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import os\n",
    "\n",
    "class GroundDataset(torch.utils.data.IterableDataset):\n",
    "\n",
    "    def __init__(self, iterator):\n",
    "        super(GroundDataset, self).__init__()\n",
    "        self._iterator = iterator\n",
    "        self._setup = False\n",
    "\n",
    "    def _setup_iterator(self):\n",
    "        self._iterator = self._iterator()\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self._setup is False:\n",
    "            self._setup_iterator()\n",
    "            self._setup = True\n",
    "        for x in self._iterator:\n",
    "            yield x\n",
    "            \n",
    "def getItr(directory):\n",
    "\n",
    "    def __iter__():\n",
    "        for ds in os.listdir(directory):\n",
    "            ds_name = os.path.join(directory,ds)\n",
    "            data = None\n",
    "            labels = None\n",
    "            for f in os.listdir(ds_name):\n",
    "                filename = os.path.join(ds_name,f)  \n",
    "                if filename.find(\"data\") > -1:\n",
    "                    data = numpy.nan_to_num(numpy.load(filename),posinf=0,neginf=0)\n",
    "                else:                   \n",
    "                    labels = numpy.nan_to_num(numpy.load(filename),posinf=0,neginf=0)\n",
    "                    print (filename,labels.mean())\n",
    "            print(data.shape)        \n",
    "            for i in range(data.shape[0]):  \n",
    "                yield numpy.transpose(data[i].astype(numpy.float64),(2,1,0)),numpy.asarray([labels[i].flatten().astype(numpy.float64).mean()]).reshape((1,1))\n",
    "    return __iter__              \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "026ab67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.000005\n",
    "\n",
    "model = AlexNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db8931cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "model_save_path = \".\"\n",
    "\n",
    "iter_ = getItr(\"processed/train\")\n",
    "train_ds =  GroundDataset(iter_)\n",
    "iter_ = getItr(\"processed/test\")\n",
    "test_ds = GroundDataset(iter_)\n",
    "iter_ = getItr(\"processed/outumn\")\n",
    "val_ds = GroundDataset(iter_)\n",
    "\n",
    "train_loss = nn.MSELoss()\n",
    "dataloader =  torch.utils.data.DataLoader(train_ds, num_workers=0, batch_size = 64, shuffle=False)\n",
    "valloader = torch.utils.data.DataLoader(val_ds, num_workers=0, batch_size = 64, shuffle=False)\n",
    "testloader = torch.utils.data.DataLoader(test_ds, num_workers=0, batch_size = 64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e2c7c6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed/train/0/T36UXA_20180621T083651centinel2_nvdi_labels.npy 0.6065357\n",
      "(2304, 228, 228, 3)\n",
      "Epoch [1/10], Step [1/899], Loss: 0.0084\n",
      "Epoch [1/10], Step [2/899], Loss: 0.0104\n",
      "Epoch [1/10], Step [3/899], Loss: 0.0077\n",
      "Epoch [1/10], Step [4/899], Loss: 0.0085\n",
      "Epoch [1/10], Step [5/899], Loss: 0.0060\n",
      "Epoch [1/10], Step [6/899], Loss: 0.0051\n",
      "Epoch [1/10], Step [7/899], Loss: 0.0033\n",
      "Epoch [1/10], Step [8/899], Loss: 0.0028\n",
      "Epoch [1/10], Step [9/899], Loss: 0.0025\n",
      "Epoch [1/10], Step [10/899], Loss: 0.0028\n",
      "Epoch [1/10], Step [11/899], Loss: 0.0028\n",
      "Epoch [1/10], Step [12/899], Loss: 0.0049\n",
      "Epoch [1/10], Step [13/899], Loss: 0.0040\n",
      "Epoch [1/10], Step [14/899], Loss: 0.0036\n",
      "Epoch [1/10], Step [15/899], Loss: 0.0022\n",
      "Epoch [1/10], Step [16/899], Loss: 0.0013\n",
      "Epoch [1/10], Step [17/899], Loss: 0.0013\n",
      "Epoch [1/10], Step [18/899], Loss: 0.0012\n",
      "Epoch [1/10], Step [19/899], Loss: 0.0011\n",
      "Epoch [1/10], Step [20/899], Loss: 0.0053\n",
      "Epoch [1/10], Step [21/899], Loss: 0.0075\n",
      "Epoch [1/10], Step [22/899], Loss: 0.0018\n",
      "Epoch [1/10], Step [23/899], Loss: 0.0011\n",
      "Epoch [1/10], Step [24/899], Loss: 0.0018\n",
      "Epoch [1/10], Step [25/899], Loss: 0.0019\n",
      "Epoch [1/10], Step [26/899], Loss: 0.0016\n",
      "Epoch [1/10], Step [27/899], Loss: 0.0051\n",
      "Epoch [1/10], Step [28/899], Loss: 0.0020\n",
      "Epoch [1/10], Step [29/899], Loss: 0.0010\n",
      "Epoch [1/10], Step [30/899], Loss: 0.0015\n",
      "Epoch [1/10], Step [31/899], Loss: 0.0046\n",
      "Epoch [1/10], Step [32/899], Loss: 0.0017\n",
      "Epoch [1/10], Step [33/899], Loss: 0.0025\n",
      "Epoch [1/10], Step [34/899], Loss: 0.0010\n",
      "Epoch [1/10], Step [35/899], Loss: 0.0012\n",
      "Epoch [1/10], Step [36/899], Loss: 0.0014\n",
      "processed/train/1/T36UXA_20180830T083601centinel2_nvdi_labels.npy 0.3716937\n",
      "(2304, 228, 228, 3)\n",
      "Epoch [1/10], Step [37/899], Loss: 0.0230\n",
      "Epoch [1/10], Step [38/899], Loss: 0.0181\n",
      "Epoch [1/10], Step [39/899], Loss: 0.0254\n",
      "Epoch [1/10], Step [40/899], Loss: 0.0101\n",
      "Epoch [1/10], Step [41/899], Loss: 0.0113\n",
      "Epoch [1/10], Step [42/899], Loss: 0.0235\n",
      "Epoch [1/10], Step [43/899], Loss: 0.0123\n",
      "Epoch [1/10], Step [44/899], Loss: 0.0097\n",
      "Epoch [1/10], Step [45/899], Loss: 0.0146\n",
      "Epoch [1/10], Step [46/899], Loss: 0.0078\n",
      "Epoch [1/10], Step [47/899], Loss: 0.0126\n",
      "Epoch [1/10], Step [48/899], Loss: 0.0056\n",
      "Epoch [1/10], Step [49/899], Loss: 0.0023\n",
      "Epoch [1/10], Step [50/899], Loss: 0.0038\n",
      "Epoch [1/10], Step [51/899], Loss: 0.0038\n",
      "Epoch [1/10], Step [52/899], Loss: 0.0058\n",
      "Epoch [1/10], Step [53/899], Loss: 0.0056\n",
      "Epoch [1/10], Step [54/899], Loss: 0.0052\n",
      "Epoch [1/10], Step [55/899], Loss: 0.0059\n",
      "Epoch [1/10], Step [56/899], Loss: 0.0056\n",
      "Epoch [1/10], Step [57/899], Loss: 0.0028\n",
      "Epoch [1/10], Step [58/899], Loss: 0.0043\n",
      "Epoch [1/10], Step [59/899], Loss: 0.0026\n",
      "Epoch [1/10], Step [60/899], Loss: 0.0031\n",
      "Epoch [1/10], Step [61/899], Loss: 0.0033\n",
      "Epoch [1/10], Step [62/899], Loss: 0.0044\n",
      "Epoch [1/10], Step [63/899], Loss: 0.0030\n",
      "Epoch [1/10], Step [64/899], Loss: 0.0016\n",
      "Epoch [1/10], Step [65/899], Loss: 0.0014\n",
      "Epoch [1/10], Step [66/899], Loss: 0.0026\n",
      "Epoch [1/10], Step [67/899], Loss: 0.0019\n",
      "Epoch [1/10], Step [68/899], Loss: 0.0019\n",
      "Epoch [1/10], Step [69/899], Loss: 0.0023\n",
      "Epoch [1/10], Step [70/899], Loss: 0.0028\n",
      "Epoch [1/10], Step [71/899], Loss: 0.0033\n",
      "Epoch [1/10], Step [72/899], Loss: 0.0066\n",
      "processed/train/2/T36UXA_20180914T083549centinel2_nvdi_labels.npy 0.30008453\n",
      "(2304, 228, 228, 3)\n",
      "Epoch [1/10], Step [73/899], Loss: 0.0045\n",
      "Epoch [1/10], Step [74/899], Loss: 0.0058\n",
      "Epoch [1/10], Step [75/899], Loss: 0.0056\n",
      "Epoch [1/10], Step [76/899], Loss: 0.0035\n",
      "Epoch [1/10], Step [77/899], Loss: 0.0025\n",
      "Epoch [1/10], Step [78/899], Loss: 0.0080\n",
      "Epoch [1/10], Step [79/899], Loss: 0.0044\n",
      "Epoch [1/10], Step [80/899], Loss: 0.0067\n",
      "Epoch [1/10], Step [81/899], Loss: 0.0107\n",
      "Epoch [1/10], Step [82/899], Loss: 0.0050\n",
      "Epoch [1/10], Step [83/899], Loss: 0.0072\n",
      "Epoch [1/10], Step [84/899], Loss: 0.0038\n",
      "Epoch [1/10], Step [85/899], Loss: 0.0017\n",
      "Epoch [1/10], Step [86/899], Loss: 0.0031\n",
      "Epoch [1/10], Step [87/899], Loss: 0.0030\n",
      "Epoch [1/10], Step [88/899], Loss: 0.0051\n",
      "Epoch [1/10], Step [89/899], Loss: 0.0039\n",
      "Epoch [1/10], Step [90/899], Loss: 0.0017\n",
      "Epoch [1/10], Step [91/899], Loss: 0.0016\n",
      "Epoch [1/10], Step [92/899], Loss: 0.0032\n",
      "Epoch [1/10], Step [93/899], Loss: 0.0026\n",
      "Epoch [1/10], Step [94/899], Loss: 0.0024\n",
      "Epoch [1/10], Step [95/899], Loss: 0.0010\n",
      "Epoch [1/10], Step [96/899], Loss: 0.0019\n",
      "Epoch [1/10], Step [97/899], Loss: 0.0025\n",
      "Epoch [1/10], Step [98/899], Loss: 0.0028\n",
      "Epoch [1/10], Step [99/899], Loss: 0.0015\n",
      "Epoch [1/10], Step [100/899], Loss: 0.0014\n",
      "Epoch [1/10], Step [101/899], Loss: 0.0023\n",
      "Epoch [1/10], Step [102/899], Loss: 0.0039\n",
      "Epoch [1/10], Step [103/899], Loss: 0.0032\n",
      "Epoch [1/10], Step [104/899], Loss: 0.0058\n",
      "Epoch [1/10], Step [105/899], Loss: 0.0025\n",
      "Epoch [1/10], Step [106/899], Loss: 0.0007\n",
      "Epoch [1/10], Step [107/899], Loss: 0.0052\n",
      "Epoch [1/10], Step [108/899], Loss: 0.0094\n",
      "processed/train/3/T36UXA_20180919T083621centinel2_nvdi_labels.npy 0.31740996\n",
      "(2304, 228, 228, 3)\n",
      "Epoch [1/10], Step [109/899], Loss: 0.0045\n",
      "Epoch [1/10], Step [110/899], Loss: 0.0084\n",
      "Epoch [1/10], Step [111/899], Loss: 0.0061\n",
      "Epoch [1/10], Step [112/899], Loss: 0.0090\n",
      "Epoch [1/10], Step [113/899], Loss: 0.0061\n",
      "Epoch [1/10], Step [114/899], Loss: 0.0026\n",
      "Epoch [1/10], Step [115/899], Loss: 0.0047\n",
      "Epoch [1/10], Step [116/899], Loss: 0.0017\n",
      "Epoch [1/10], Step [117/899], Loss: 0.0031\n",
      "Epoch [1/10], Step [118/899], Loss: 0.0011\n",
      "Epoch [1/10], Step [119/899], Loss: 0.0031\n",
      "Epoch [1/10], Step [120/899], Loss: 0.0016\n",
      "Epoch [1/10], Step [121/899], Loss: 0.0016\n",
      "Epoch [1/10], Step [122/899], Loss: 0.0046\n",
      "Epoch [1/10], Step [123/899], Loss: 0.0030\n",
      "Epoch [1/10], Step [124/899], Loss: 0.0046\n",
      "Epoch [1/10], Step [125/899], Loss: 0.0026\n",
      "Epoch [1/10], Step [126/899], Loss: 0.0012\n",
      "Epoch [1/10], Step [127/899], Loss: 0.0011\n",
      "Epoch [1/10], Step [128/899], Loss: 0.0017\n",
      "Epoch [1/10], Step [129/899], Loss: 0.0028\n",
      "Epoch [1/10], Step [130/899], Loss: 0.0012\n",
      "Epoch [1/10], Step [131/899], Loss: 0.0006\n",
      "Epoch [1/10], Step [132/899], Loss: 0.0011\n",
      "Epoch [1/10], Step [133/899], Loss: 0.0014\n",
      "Epoch [1/10], Step [134/899], Loss: 0.0018\n",
      "Epoch [1/10], Step [135/899], Loss: 0.0011\n",
      "Epoch [1/10], Step [136/899], Loss: 0.0012\n",
      "Epoch [1/10], Step [137/899], Loss: 0.0031\n",
      "Epoch [1/10], Step [138/899], Loss: 0.0053\n",
      "Epoch [1/10], Step [139/899], Loss: 0.0037\n",
      "Epoch [1/10], Step [140/899], Loss: 0.0081\n",
      "Epoch [1/10], Step [141/899], Loss: 0.0038\n",
      "Epoch [1/10], Step [142/899], Loss: 0.0013\n",
      "Epoch [1/10], Step [143/899], Loss: 0.0062\n",
      "Epoch [1/10], Step [144/899], Loss: 0.0076\n",
      "processed/train/4/T36UXB_20180415T084601centinel2_nvdi_labels.npy 0.17540433\n",
      "(2304, 228, 228, 3)\n",
      "Epoch [1/10], Step [145/899], Loss: 0.0357\n",
      "Epoch [1/10], Step [146/899], Loss: 0.0450\n",
      "Epoch [1/10], Step [147/899], Loss: 0.0472\n",
      "Epoch [1/10], Step [148/899], Loss: 0.0415\n",
      "Epoch [1/10], Step [149/899], Loss: 0.0251\n",
      "Epoch [1/10], Step [150/899], Loss: 0.0184\n",
      "Epoch [1/10], Step [151/899], Loss: 0.0120\n",
      "Epoch [1/10], Step [152/899], Loss: 0.0152\n",
      "Epoch [1/10], Step [153/899], Loss: 0.0158\n",
      "Epoch [1/10], Step [154/899], Loss: 0.0121\n",
      "Epoch [1/10], Step [155/899], Loss: 0.0085\n",
      "Epoch [1/10], Step [156/899], Loss: 0.0051\n",
      "Epoch [1/10], Step [157/899], Loss: 0.0046\n",
      "Epoch [1/10], Step [158/899], Loss: 0.0091\n",
      "Epoch [1/10], Step [159/899], Loss: 0.0048\n",
      "Epoch [1/10], Step [160/899], Loss: 0.0044\n",
      "Epoch [1/10], Step [161/899], Loss: 0.0024\n",
      "Epoch [1/10], Step [162/899], Loss: 0.0025\n",
      "Epoch [1/10], Step [163/899], Loss: 0.0027\n",
      "Epoch [1/10], Step [164/899], Loss: 0.0021\n",
      "Epoch [1/10], Step [165/899], Loss: 0.0013\n",
      "Epoch [1/10], Step [166/899], Loss: 0.0014\n",
      "Epoch [1/10], Step [167/899], Loss: 0.0028\n",
      "Epoch [1/10], Step [168/899], Loss: 0.0011\n",
      "Epoch [1/10], Step [169/899], Loss: 0.0022\n",
      "Epoch [1/10], Step [170/899], Loss: 0.0017\n",
      "Epoch [1/10], Step [171/899], Loss: 0.0020\n",
      "Epoch [1/10], Step [172/899], Loss: 0.0047\n",
      "Epoch [1/10], Step [173/899], Loss: 0.0021\n",
      "Epoch [1/10], Step [174/899], Loss: 0.0033\n",
      "Epoch [1/10], Step [175/899], Loss: 0.0032\n",
      "Epoch [1/10], Step [176/899], Loss: 0.0029\n",
      "Epoch [1/10], Step [177/899], Loss: 0.0033\n",
      "Epoch [1/10], Step [178/899], Loss: 0.0032\n",
      "Epoch [1/10], Step [179/899], Loss: 0.0041\n",
      "Epoch [1/10], Step [180/899], Loss: 0.0039\n",
      "processed/train/6/T36UXB_20180917T084609centinel2_nvdi_labels.npy 0.3902404\n",
      "(2304, 228, 228, 3)\n",
      "Epoch [1/10], Step [181/899], Loss: 0.0644\n",
      "Epoch [1/10], Step [182/899], Loss: 0.0622\n",
      "Epoch [1/10], Step [183/899], Loss: 0.0570\n",
      "Epoch [1/10], Step [184/899], Loss: 0.0658\n",
      "Epoch [1/10], Step [185/899], Loss: 0.0391\n",
      "Epoch [1/10], Step [186/899], Loss: 0.0364\n",
      "Epoch [1/10], Step [187/899], Loss: 0.0253\n",
      "Epoch [1/10], Step [188/899], Loss: 0.0309\n",
      "Epoch [1/10], Step [189/899], Loss: 0.0458\n",
      "Epoch [1/10], Step [190/899], Loss: 0.0207\n",
      "Epoch [1/10], Step [191/899], Loss: 0.0363\n",
      "Epoch [1/10], Step [192/899], Loss: 0.0595\n",
      "Epoch [1/10], Step [193/899], Loss: 0.0356\n",
      "Epoch [1/10], Step [194/899], Loss: 0.0528\n",
      "Epoch [1/10], Step [195/899], Loss: 0.0338\n",
      "Epoch [1/10], Step [196/899], Loss: 0.0294\n",
      "Epoch [1/10], Step [197/899], Loss: 0.0244\n",
      "Epoch [1/10], Step [198/899], Loss: 0.0124\n",
      "Epoch [1/10], Step [199/899], Loss: 0.0173\n",
      "Epoch [1/10], Step [200/899], Loss: 0.0257\n",
      "Epoch [1/10], Step [201/899], Loss: 0.0107\n",
      "Epoch [1/10], Step [202/899], Loss: 0.0086\n",
      "Epoch [1/10], Step [203/899], Loss: 0.0138\n",
      "Epoch [1/10], Step [204/899], Loss: 0.0035\n",
      "Epoch [1/10], Step [205/899], Loss: 0.0091\n",
      "Epoch [1/10], Step [206/899], Loss: 0.0038\n",
      "Epoch [1/10], Step [207/899], Loss: 0.0026\n",
      "Epoch [1/10], Step [208/899], Loss: 0.0037\n",
      "Epoch [1/10], Step [209/899], Loss: 0.0015\n",
      "Epoch [1/10], Step [210/899], Loss: 0.0018\n",
      "Epoch [1/10], Step [211/899], Loss: 0.0026\n",
      "Epoch [1/10], Step [212/899], Loss: 0.0026\n",
      "Epoch [1/10], Step [213/899], Loss: 0.0038\n",
      "Epoch [1/10], Step [214/899], Loss: 0.0051\n",
      "Epoch [1/10], Step [215/899], Loss: 0.0016\n",
      "Epoch [1/10], Step [216/899], Loss: 0.0037\n",
      "processed/train/7/T36UYA_20180611T083601centinel2_nvdi_labels.npy 0.5656771\n",
      "(2304, 228, 228, 3)\n",
      "Epoch [1/10], Step [217/899], Loss: 0.0284\n",
      "Epoch [1/10], Step [218/899], Loss: 0.0269\n",
      "Epoch [1/10], Step [219/899], Loss: 0.0208\n",
      "Epoch [1/10], Step [220/899], Loss: 0.0104\n",
      "Epoch [1/10], Step [221/899], Loss: 0.0151\n",
      "Epoch [1/10], Step [222/899], Loss: 0.0208\n",
      "Epoch [1/10], Step [223/899], Loss: 0.0136\n",
      "Epoch [1/10], Step [224/899], Loss: 0.0088\n",
      "Epoch [1/10], Step [225/899], Loss: 0.0132\n",
      "Epoch [1/10], Step [226/899], Loss: 0.0092\n",
      "Epoch [1/10], Step [227/899], Loss: 0.0085\n",
      "Epoch [1/10], Step [228/899], Loss: 0.0102\n",
      "Epoch [1/10], Step [229/899], Loss: 0.0131\n",
      "Epoch [1/10], Step [230/899], Loss: 0.0074\n",
      "Epoch [1/10], Step [231/899], Loss: 0.0057\n",
      "Epoch [1/10], Step [232/899], Loss: 0.0050\n",
      "Epoch [1/10], Step [233/899], Loss: 0.0035\n",
      "Epoch [1/10], Step [234/899], Loss: 0.0078\n",
      "Epoch [1/10], Step [235/899], Loss: 0.0111\n",
      "Epoch [1/10], Step [236/899], Loss: 0.0086\n",
      "Epoch [1/10], Step [237/899], Loss: 0.0159\n",
      "Epoch [1/10], Step [238/899], Loss: 0.0062\n",
      "Epoch [1/10], Step [239/899], Loss: 0.0030\n",
      "Epoch [1/10], Step [240/899], Loss: 0.0052\n",
      "Epoch [1/10], Step [241/899], Loss: 0.0019\n",
      "Epoch [1/10], Step [242/899], Loss: 0.0022\n",
      "Epoch [1/10], Step [243/899], Loss: 0.0041\n",
      "Epoch [1/10], Step [244/899], Loss: 0.0027\n",
      "Epoch [1/10], Step [245/899], Loss: 0.0064\n",
      "Epoch [1/10], Step [246/899], Loss: 0.0020\n",
      "Epoch [1/10], Step [247/899], Loss: 0.0017\n",
      "Epoch [1/10], Step [248/899], Loss: 0.0029\n",
      "Epoch [1/10], Step [249/899], Loss: 0.0020\n",
      "Epoch [1/10], Step [250/899], Loss: 0.0027\n",
      "Epoch [1/10], Step [251/899], Loss: 0.0131\n",
      "Epoch [1/10], Step [252/899], Loss: 0.0018\n",
      "processed/train/9/T36UYA_20180825T083549centinel2_nvdi_labels.npy 0.40812686\n",
      "(2304, 228, 228, 3)\n",
      "Epoch [1/10], Step [253/899], Loss: 0.0120\n",
      "Epoch [1/10], Step [254/899], Loss: 0.0165\n",
      "Epoch [1/10], Step [255/899], Loss: 0.0101\n",
      "Epoch [1/10], Step [256/899], Loss: 0.0064\n",
      "Epoch [1/10], Step [257/899], Loss: 0.0076\n",
      "Epoch [1/10], Step [258/899], Loss: 0.0071\n",
      "Epoch [1/10], Step [259/899], Loss: 0.0062\n",
      "Epoch [1/10], Step [260/899], Loss: 0.0036\n",
      "Epoch [1/10], Step [261/899], Loss: 0.0051\n",
      "Epoch [1/10], Step [262/899], Loss: 0.0051\n",
      "Epoch [1/10], Step [263/899], Loss: 0.0031\n",
      "Epoch [1/10], Step [264/899], Loss: 0.0018\n",
      "Epoch [1/10], Step [265/899], Loss: 0.0029\n",
      "Epoch [1/10], Step [266/899], Loss: 0.0029\n",
      "Epoch [1/10], Step [267/899], Loss: 0.0036\n",
      "Epoch [1/10], Step [268/899], Loss: 0.0086\n",
      "Epoch [1/10], Step [269/899], Loss: 0.0092\n",
      "Epoch [1/10], Step [270/899], Loss: 0.0115\n",
      "Epoch [1/10], Step [271/899], Loss: 0.0128\n",
      "Epoch [1/10], Step [272/899], Loss: 0.0063\n",
      "Epoch [1/10], Step [273/899], Loss: 0.0220\n",
      "Epoch [1/10], Step [274/899], Loss: 0.0154\n",
      "Epoch [1/10], Step [275/899], Loss: 0.0037\n",
      "Epoch [1/10], Step [276/899], Loss: 0.0039\n",
      "Epoch [1/10], Step [277/899], Loss: 0.0031\n",
      "Epoch [1/10], Step [278/899], Loss: 0.0039\n",
      "Epoch [1/10], Step [279/899], Loss: 0.0032\n",
      "Epoch [1/10], Step [280/899], Loss: 0.0022\n",
      "Epoch [1/10], Step [281/899], Loss: 0.0018\n",
      "Epoch [1/10], Step [282/899], Loss: 0.0073\n",
      "Epoch [1/10], Step [283/899], Loss: 0.0060\n",
      "Epoch [1/10], Step [284/899], Loss: 0.0071\n",
      "Epoch [1/10], Step [285/899], Loss: 0.0037\n",
      "Epoch [1/10], Step [286/899], Loss: 0.0021\n",
      "Epoch [1/10], Step [287/899], Loss: 0.0171\n",
      "Epoch [1/10], Step [288/899], Loss: 0.0012\n",
      "processed/train/10/T36UYA_20180904T083549centinel2_nvdi_labels.npy 0.3330228\n",
      "(2304, 228, 228, 3)\n",
      "Epoch [1/10], Step [289/899], Loss: 0.0051\n",
      "Epoch [1/10], Step [290/899], Loss: 0.0067\n",
      "Epoch [1/10], Step [291/899], Loss: 0.0033\n",
      "Epoch [1/10], Step [292/899], Loss: 0.0040\n",
      "Epoch [1/10], Step [293/899], Loss: 0.0039\n",
      "Epoch [1/10], Step [294/899], Loss: 0.0032\n",
      "Epoch [1/10], Step [295/899], Loss: 0.0038\n",
      "Epoch [1/10], Step [296/899], Loss: 0.0021\n",
      "Epoch [1/10], Step [297/899], Loss: 0.0033\n",
      "Epoch [1/10], Step [298/899], Loss: 0.0042\n",
      "Epoch [1/10], Step [299/899], Loss: 0.0015\n",
      "Epoch [1/10], Step [300/899], Loss: 0.0009\n",
      "Epoch [1/10], Step [301/899], Loss: 0.0030\n",
      "Epoch [1/10], Step [302/899], Loss: 0.0021\n",
      "Epoch [1/10], Step [303/899], Loss: 0.0028\n",
      "Epoch [1/10], Step [304/899], Loss: 0.0049\n",
      "Epoch [1/10], Step [305/899], Loss: 0.0055\n",
      "Epoch [1/10], Step [306/899], Loss: 0.0084\n",
      "Epoch [1/10], Step [307/899], Loss: 0.0087\n",
      "Epoch [1/10], Step [308/899], Loss: 0.0045\n",
      "Epoch [1/10], Step [309/899], Loss: 0.0146\n",
      "Epoch [1/10], Step [310/899], Loss: 0.0107\n",
      "Epoch [1/10], Step [311/899], Loss: 0.0020\n",
      "Epoch [1/10], Step [312/899], Loss: 0.0025\n",
      "Epoch [1/10], Step [313/899], Loss: 0.0027\n",
      "Epoch [1/10], Step [314/899], Loss: 0.0031\n",
      "Epoch [1/10], Step [315/899], Loss: 0.0036\n",
      "Epoch [1/10], Step [316/899], Loss: 0.0020\n",
      "Epoch [1/10], Step [317/899], Loss: 0.0013\n",
      "Epoch [1/10], Step [318/899], Loss: 0.0052\n",
      "Epoch [1/10], Step [319/899], Loss: 0.0054\n",
      "Epoch [1/10], Step [320/899], Loss: 0.0073\n",
      "Epoch [1/10], Step [321/899], Loss: 0.0040\n",
      "Epoch [1/10], Step [322/899], Loss: 0.0020\n",
      "Epoch [1/10], Step [323/899], Loss: 0.0120\n",
      "Epoch [1/10], Step [324/899], Loss: 0.0008\n",
      "processed/train/11/T36UYA_20180919T083621centinel2_nvdi_labels.npy 0.3254697\n",
      "(2304, 228, 228, 3)\n",
      "Epoch [1/10], Step [325/899], Loss: 0.0017\n",
      "Epoch [1/10], Step [326/899], Loss: 0.0057\n",
      "Epoch [1/10], Step [327/899], Loss: 0.0020\n",
      "Epoch [1/10], Step [328/899], Loss: 0.0022\n",
      "Epoch [1/10], Step [329/899], Loss: 0.0009\n",
      "Epoch [1/10], Step [330/899], Loss: 0.0016\n",
      "Epoch [1/10], Step [331/899], Loss: 0.0012\n",
      "Epoch [1/10], Step [332/899], Loss: 0.0010\n",
      "Epoch [1/10], Step [333/899], Loss: 0.0018\n",
      "Epoch [1/10], Step [334/899], Loss: 0.0025\n",
      "Epoch [1/10], Step [335/899], Loss: 0.0038\n",
      "Epoch [1/10], Step [336/899], Loss: 0.0015\n",
      "Epoch [1/10], Step [337/899], Loss: 0.0033\n",
      "Epoch [1/10], Step [338/899], Loss: 0.0043\n",
      "Epoch [1/10], Step [339/899], Loss: 0.0031\n",
      "Epoch [1/10], Step [340/899], Loss: 0.0062\n",
      "Epoch [1/10], Step [341/899], Loss: 0.0066\n",
      "Epoch [1/10], Step [342/899], Loss: 0.0114\n",
      "Epoch [1/10], Step [343/899], Loss: 0.0105\n",
      "Epoch [1/10], Step [344/899], Loss: 0.0043\n",
      "Epoch [1/10], Step [345/899], Loss: 0.0178\n",
      "Epoch [1/10], Step [346/899], Loss: 0.0159\n",
      "Epoch [1/10], Step [347/899], Loss: 0.0027\n",
      "Epoch [1/10], Step [348/899], Loss: 0.0023\n",
      "Epoch [1/10], Step [349/899], Loss: 0.0039\n",
      "Epoch [1/10], Step [350/899], Loss: 0.0031\n",
      "Epoch [1/10], Step [351/899], Loss: 0.0031\n",
      "Epoch [1/10], Step [352/899], Loss: 0.0020\n",
      "Epoch [1/10], Step [353/899], Loss: 0.0014\n",
      "Epoch [1/10], Step [354/899], Loss: 0.0077\n",
      "Epoch [1/10], Step [355/899], Loss: 0.0085\n",
      "Epoch [1/10], Step [356/899], Loss: 0.0097\n",
      "Epoch [1/10], Step [357/899], Loss: 0.0056\n",
      "Epoch [1/10], Step [358/899], Loss: 0.0021\n",
      "Epoch [1/10], Step [359/899], Loss: 0.0133\n",
      "Epoch [1/10], Step [360/899], Loss: 0.0017\n",
      "processed/train/12/T36UYB_20180621T083651centinel2_nvdi_labels.npy 0.60621893\n",
      "(2304, 228, 228, 3)\n",
      "Epoch [1/10], Step [361/899], Loss: 0.0718\n",
      "Epoch [1/10], Step [362/899], Loss: 0.0832\n",
      "Epoch [1/10], Step [363/899], Loss: 0.0796\n",
      "Epoch [1/10], Step [364/899], Loss: 0.0580\n",
      "Epoch [1/10], Step [365/899], Loss: 0.0568\n",
      "Epoch [1/10], Step [366/899], Loss: 0.0443\n",
      "Epoch [1/10], Step [367/899], Loss: 0.0367\n",
      "Epoch [1/10], Step [368/899], Loss: 0.0347\n",
      "Epoch [1/10], Step [369/899], Loss: 0.0210\n",
      "Epoch [1/10], Step [370/899], Loss: 0.0236\n",
      "Epoch [1/10], Step [371/899], Loss: 0.0156\n",
      "Epoch [1/10], Step [372/899], Loss: 0.0085\n",
      "Epoch [1/10], Step [373/899], Loss: 0.0086\n",
      "Epoch [1/10], Step [374/899], Loss: 0.0112\n",
      "Epoch [1/10], Step [375/899], Loss: 0.0103\n",
      "Epoch [1/10], Step [376/899], Loss: 0.0083\n",
      "Epoch [1/10], Step [377/899], Loss: 0.0075\n",
      "Epoch [1/10], Step [378/899], Loss: 0.0170\n",
      "Epoch [1/10], Step [379/899], Loss: 0.0118\n",
      "Epoch [1/10], Step [380/899], Loss: 0.0178\n",
      "Epoch [1/10], Step [381/899], Loss: 0.0141\n",
      "Epoch [1/10], Step [382/899], Loss: 0.0098\n",
      "Epoch [1/10], Step [383/899], Loss: 0.0077\n",
      "Epoch [1/10], Step [384/899], Loss: 0.0063\n",
      "Epoch [1/10], Step [385/899], Loss: 0.0031\n",
      "Epoch [1/10], Step [386/899], Loss: 0.0019\n",
      "Epoch [1/10], Step [387/899], Loss: 0.0031\n",
      "Epoch [1/10], Step [388/899], Loss: 0.0019\n",
      "Epoch [1/10], Step [389/899], Loss: 0.0036\n",
      "Epoch [1/10], Step [390/899], Loss: 0.0033\n",
      "Epoch [1/10], Step [391/899], Loss: 0.0049\n",
      "Epoch [1/10], Step [392/899], Loss: 0.0030\n",
      "Epoch [1/10], Step [393/899], Loss: 0.0092\n",
      "Epoch [1/10], Step [394/899], Loss: 0.0050\n",
      "Epoch [1/10], Step [395/899], Loss: 0.0100\n",
      "Epoch [1/10], Step [396/899], Loss: 0.0031\n",
      "processed/train/13/T36UYB_20180820T083601centinel2_nvdi_labels.npy 0.5191111\n",
      "(2304, 228, 228, 3)\n",
      "Epoch [1/10], Step [397/899], Loss: 0.0073\n",
      "Epoch [1/10], Step [398/899], Loss: 0.0150\n",
      "Epoch [1/10], Step [399/899], Loss: 0.0108\n",
      "Epoch [1/10], Step [400/899], Loss: 0.0100\n",
      "Epoch [1/10], Step [401/899], Loss: 0.0042\n",
      "Epoch [1/10], Step [402/899], Loss: 0.0093\n",
      "Epoch [1/10], Step [403/899], Loss: 0.0032\n",
      "Epoch [1/10], Step [404/899], Loss: 0.0028\n",
      "Epoch [1/10], Step [405/899], Loss: 0.0036\n",
      "Epoch [1/10], Step [406/899], Loss: 0.0015\n",
      "Epoch [1/10], Step [407/899], Loss: 0.0018\n",
      "Epoch [1/10], Step [408/899], Loss: 0.0015\n",
      "Epoch [1/10], Step [409/899], Loss: 0.0022\n",
      "Epoch [1/10], Step [410/899], Loss: 0.0033\n",
      "Epoch [1/10], Step [411/899], Loss: 0.0032\n",
      "Epoch [1/10], Step [412/899], Loss: 0.0049\n",
      "Epoch [1/10], Step [413/899], Loss: 0.0059\n",
      "Epoch [1/10], Step [414/899], Loss: 0.0121\n",
      "Epoch [1/10], Step [415/899], Loss: 0.0078\n",
      "Epoch [1/10], Step [416/899], Loss: 0.0054\n",
      "Epoch [1/10], Step [417/899], Loss: 0.0016\n",
      "Epoch [1/10], Step [418/899], Loss: 0.0024\n",
      "Epoch [1/10], Step [419/899], Loss: 0.0021\n",
      "Epoch [1/10], Step [420/899], Loss: 0.0017\n",
      "Epoch [1/10], Step [421/899], Loss: 0.0024\n",
      "Epoch [1/10], Step [422/899], Loss: 0.0039\n",
      "Epoch [1/10], Step [423/899], Loss: 0.0038\n",
      "Epoch [1/10], Step [424/899], Loss: 0.0025\n",
      "Epoch [1/10], Step [425/899], Loss: 0.0026\n",
      "Epoch [1/10], Step [426/899], Loss: 0.0019\n",
      "Epoch [1/10], Step [427/899], Loss: 0.0013\n",
      "Epoch [1/10], Step [428/899], Loss: 0.0008\n",
      "Epoch [1/10], Step [429/899], Loss: 0.0033\n",
      "Epoch [1/10], Step [430/899], Loss: 0.0095\n",
      "Epoch [1/10], Step [431/899], Loss: 0.0191\n",
      "Epoch [1/10], Step [432/899], Loss: 0.0116\n",
      "processed/train/14/T36UYB_20180825T083549centinel2_nvdi_labels.npy 0.49511558\n",
      "(2304, 228, 228, 3)\n",
      "Epoch [1/10], Step [433/899], Loss: 0.0047\n",
      "Epoch [1/10], Step [434/899], Loss: 0.0105\n",
      "Epoch [1/10], Step [435/899], Loss: 0.0089\n",
      "Epoch [1/10], Step [436/899], Loss: 0.0060\n",
      "Epoch [1/10], Step [437/899], Loss: 0.0014\n",
      "Epoch [1/10], Step [438/899], Loss: 0.0067\n",
      "Epoch [1/10], Step [439/899], Loss: 0.0015\n",
      "Epoch [1/10], Step [440/899], Loss: 0.0019\n",
      "Epoch [1/10], Step [441/899], Loss: 0.0018\n",
      "Epoch [1/10], Step [442/899], Loss: 0.0019\n",
      "Epoch [1/10], Step [443/899], Loss: 0.0018\n",
      "Epoch [1/10], Step [444/899], Loss: 0.0011\n",
      "Epoch [1/10], Step [445/899], Loss: 0.0016\n",
      "Epoch [1/10], Step [446/899], Loss: 0.0030\n",
      "Epoch [1/10], Step [447/899], Loss: 0.0026\n",
      "Epoch [1/10], Step [448/899], Loss: 0.0052\n",
      "Epoch [1/10], Step [449/899], Loss: 0.0063\n",
      "Epoch [1/10], Step [450/899], Loss: 0.0114\n",
      "Epoch [1/10], Step [451/899], Loss: 0.0071\n",
      "Epoch [1/10], Step [452/899], Loss: 0.0042\n",
      "Epoch [1/10], Step [453/899], Loss: 0.0010\n",
      "Epoch [1/10], Step [454/899], Loss: 0.0022\n",
      "Epoch [1/10], Step [455/899], Loss: 0.0016\n",
      "Epoch [1/10], Step [456/899], Loss: 0.0015\n",
      "Epoch [1/10], Step [457/899], Loss: 0.0030\n",
      "Epoch [1/10], Step [458/899], Loss: 0.0040\n",
      "Epoch [1/10], Step [459/899], Loss: 0.0026\n",
      "Epoch [1/10], Step [460/899], Loss: 0.0027\n",
      "Epoch [1/10], Step [461/899], Loss: 0.0027\n",
      "Epoch [1/10], Step [462/899], Loss: 0.0022\n",
      "Epoch [1/10], Step [463/899], Loss: 0.0011\n",
      "Epoch [1/10], Step [464/899], Loss: 0.0006\n",
      "Epoch [1/10], Step [465/899], Loss: 0.0029\n",
      "Epoch [1/10], Step [466/899], Loss: 0.0120\n",
      "Epoch [1/10], Step [467/899], Loss: 0.0188\n",
      "Epoch [1/10], Step [468/899], Loss: 0.0119\n",
      "processed/train/15/T36UYB_20180904T083549centinel2_nvdi_labels.npy 0.3942621\n",
      "(2304, 228, 228, 3)\n",
      "Epoch [1/10], Step [469/899], Loss: 0.0288\n",
      "Epoch [1/10], Step [470/899], Loss: 0.0324\n",
      "Epoch [1/10], Step [471/899], Loss: 0.0283\n",
      "Epoch [1/10], Step [472/899], Loss: 0.0181\n",
      "Epoch [1/10], Step [473/899], Loss: 0.0070\n",
      "Epoch [1/10], Step [474/899], Loss: 0.0141\n",
      "Epoch [1/10], Step [475/899], Loss: 0.0034\n",
      "Epoch [1/10], Step [476/899], Loss: 0.0039\n",
      "Epoch [1/10], Step [477/899], Loss: 0.0024\n",
      "Epoch [1/10], Step [478/899], Loss: 0.0015\n",
      "Epoch [1/10], Step [479/899], Loss: 0.0012\n",
      "Epoch [1/10], Step [480/899], Loss: 0.0012\n",
      "Epoch [1/10], Step [481/899], Loss: 0.0014\n",
      "Epoch [1/10], Step [482/899], Loss: 0.0014\n",
      "Epoch [1/10], Step [483/899], Loss: 0.0017\n",
      "Epoch [1/10], Step [484/899], Loss: 0.0023\n",
      "Epoch [1/10], Step [485/899], Loss: 0.0038\n",
      "Epoch [1/10], Step [486/899], Loss: 0.0095\n",
      "Epoch [1/10], Step [487/899], Loss: 0.0064\n",
      "Epoch [1/10], Step [488/899], Loss: 0.0051\n",
      "Epoch [1/10], Step [489/899], Loss: 0.0032\n",
      "Epoch [1/10], Step [490/899], Loss: 0.0044\n",
      "Epoch [1/10], Step [491/899], Loss: 0.0059\n",
      "Epoch [1/10], Step [492/899], Loss: 0.0060\n",
      "Epoch [1/10], Step [493/899], Loss: 0.0122\n",
      "Epoch [1/10], Step [494/899], Loss: 0.0113\n",
      "Epoch [1/10], Step [495/899], Loss: 0.0080\n",
      "Epoch [1/10], Step [496/899], Loss: 0.0088\n",
      "Epoch [1/10], Step [497/899], Loss: 0.0075\n",
      "Epoch [1/10], Step [498/899], Loss: 0.0068\n",
      "Epoch [1/10], Step [499/899], Loss: 0.0047\n",
      "Epoch [1/10], Step [500/899], Loss: 0.0028\n",
      "Epoch [1/10], Step [501/899], Loss: 0.0017\n",
      "Epoch [1/10], Step [502/899], Loss: 0.0047\n",
      "Epoch [1/10], Step [503/899], Loss: 0.0087\n",
      "Epoch [1/10], Step [504/899], Loss: 0.0054\n",
      "processed/train/16/T36UYB_20180919T083621centinel2_nvdi_labels.npy 0.35286027\n",
      "(2304, 228, 228, 3)\n",
      "Epoch [1/10], Step [505/899], Loss: 0.0135\n",
      "Epoch [1/10], Step [506/899], Loss: 0.0148\n",
      "Epoch [1/10], Step [507/899], Loss: 0.0145\n",
      "Epoch [1/10], Step [508/899], Loss: 0.0130\n",
      "Epoch [1/10], Step [509/899], Loss: 0.0078\n",
      "Epoch [1/10], Step [510/899], Loss: 0.0111\n",
      "Epoch [1/10], Step [511/899], Loss: 0.0051\n",
      "Epoch [1/10], Step [512/899], Loss: 0.0068\n",
      "Epoch [1/10], Step [513/899], Loss: 0.0047\n",
      "Epoch [1/10], Step [514/899], Loss: 0.0025\n",
      "Epoch [1/10], Step [515/899], Loss: 0.0045\n",
      "Epoch [1/10], Step [516/899], Loss: 0.0035\n",
      "Epoch [1/10], Step [517/899], Loss: 0.0024\n",
      "Epoch [1/10], Step [518/899], Loss: 0.0023\n",
      "Epoch [1/10], Step [519/899], Loss: 0.0027\n",
      "Epoch [1/10], Step [520/899], Loss: 0.0017\n",
      "Epoch [1/10], Step [521/899], Loss: 0.0012\n",
      "Epoch [1/10], Step [522/899], Loss: 0.0019\n",
      "Epoch [1/10], Step [523/899], Loss: 0.0037\n",
      "Epoch [1/10], Step [524/899], Loss: 0.0017\n",
      "Epoch [1/10], Step [525/899], Loss: 0.0015\n",
      "Epoch [1/10], Step [526/899], Loss: 0.0019\n",
      "Epoch [1/10], Step [527/899], Loss: 0.0042\n",
      "Epoch [1/10], Step [528/899], Loss: 0.0040\n",
      "Epoch [1/10], Step [529/899], Loss: 0.0132\n",
      "Epoch [1/10], Step [530/899], Loss: 0.0116\n",
      "Epoch [1/10], Step [531/899], Loss: 0.0100\n",
      "Epoch [1/10], Step [532/899], Loss: 0.0121\n",
      "Epoch [1/10], Step [533/899], Loss: 0.0108\n",
      "Epoch [1/10], Step [534/899], Loss: 0.0086\n",
      "Epoch [1/10], Step [535/899], Loss: 0.0088\n",
      "Epoch [1/10], Step [536/899], Loss: 0.0043\n",
      "Epoch [1/10], Step [537/899], Loss: 0.0051\n",
      "Epoch [1/10], Step [538/899], Loss: 0.0023\n",
      "Epoch [1/10], Step [539/899], Loss: 0.0072\n",
      "Epoch [1/10], Step [540/899], Loss: 0.0025\n",
      "processed/train/17/T37UCR_20180502T083601centinel2_nvdi_labels.npy 0.34315002\n",
      "(2304, 228, 228, 3)\n",
      "Epoch [1/10], Step [541/899], Loss: 0.0011\n",
      "Epoch [1/10], Step [542/899], Loss: 0.0029\n",
      "Epoch [1/10], Step [543/899], Loss: 0.0052\n",
      "Epoch [1/10], Step [544/899], Loss: 0.0061\n",
      "Epoch [1/10], Step [545/899], Loss: 0.0033\n",
      "Epoch [1/10], Step [546/899], Loss: 0.0036\n",
      "Epoch [1/10], Step [547/899], Loss: 0.0024\n",
      "Epoch [1/10], Step [548/899], Loss: 0.0013\n",
      "Epoch [1/10], Step [549/899], Loss: 0.0077\n",
      "Epoch [1/10], Step [550/899], Loss: 0.0044\n",
      "Epoch [1/10], Step [551/899], Loss: 0.0025\n",
      "Epoch [1/10], Step [552/899], Loss: 0.0026\n",
      "Epoch [1/10], Step [553/899], Loss: 0.0029\n",
      "Epoch [1/10], Step [554/899], Loss: 0.0030\n",
      "Epoch [1/10], Step [555/899], Loss: 0.0020\n",
      "Epoch [1/10], Step [556/899], Loss: 0.0018\n",
      "Epoch [1/10], Step [557/899], Loss: 0.0031\n",
      "Epoch [1/10], Step [558/899], Loss: 0.0016\n",
      "Epoch [1/10], Step [559/899], Loss: 0.0017\n",
      "Epoch [1/10], Step [560/899], Loss: 0.0103\n",
      "Epoch [1/10], Step [561/899], Loss: 0.0036\n",
      "Epoch [1/10], Step [562/899], Loss: 0.0036\n",
      "Epoch [1/10], Step [563/899], Loss: 0.0138\n",
      "Epoch [1/10], Step [564/899], Loss: 0.0035\n",
      "Epoch [1/10], Step [565/899], Loss: 0.0017\n",
      "Epoch [1/10], Step [566/899], Loss: 0.0034\n",
      "Epoch [1/10], Step [567/899], Loss: 0.0024\n",
      "Epoch [1/10], Step [568/899], Loss: 0.0091\n",
      "Epoch [1/10], Step [569/899], Loss: 0.0023\n",
      "Epoch [1/10], Step [570/899], Loss: 0.0027\n",
      "Epoch [1/10], Step [571/899], Loss: 0.0019\n",
      "Epoch [1/10], Step [572/899], Loss: 0.0048\n",
      "Epoch [1/10], Step [573/899], Loss: 0.0011\n",
      "Epoch [1/10], Step [574/899], Loss: 0.0044\n",
      "Epoch [1/10], Step [575/899], Loss: 0.0079\n",
      "Epoch [1/10], Step [576/899], Loss: 0.0044\n",
      "processed/train/18/T37UCR_20180611T083601centinel2_nvdi_labels.npy 0.56338584\n",
      "(2304, 228, 228, 3)\n",
      "Epoch [1/10], Step [577/899], Loss: 0.0499\n",
      "Epoch [1/10], Step [578/899], Loss: 0.0765\n",
      "Epoch [1/10], Step [579/899], Loss: 0.0560\n",
      "Epoch [1/10], Step [580/899], Loss: 0.0532\n",
      "Epoch [1/10], Step [581/899], Loss: 0.0455\n",
      "Epoch [1/10], Step [582/899], Loss: 0.0436\n",
      "Epoch [1/10], Step [583/899], Loss: 0.0332\n",
      "Epoch [1/10], Step [584/899], Loss: 0.0511\n",
      "Epoch [1/10], Step [585/899], Loss: 0.0212\n",
      "Epoch [1/10], Step [586/899], Loss: 0.0204\n",
      "Epoch [1/10], Step [587/899], Loss: 0.0169\n",
      "Epoch [1/10], Step [588/899], Loss: 0.0080\n",
      "Epoch [1/10], Step [589/899], Loss: 0.0138\n",
      "Epoch [1/10], Step [590/899], Loss: 0.0084\n",
      "Epoch [1/10], Step [591/899], Loss: 0.0065\n",
      "Epoch [1/10], Step [592/899], Loss: 0.0076\n",
      "Epoch [1/10], Step [593/899], Loss: 0.0059\n",
      "Epoch [1/10], Step [594/899], Loss: 0.0095\n",
      "Epoch [1/10], Step [595/899], Loss: 0.0160\n",
      "Epoch [1/10], Step [596/899], Loss: 0.0145\n",
      "Epoch [1/10], Step [597/899], Loss: 0.0130\n",
      "Epoch [1/10], Step [598/899], Loss: 0.0090\n",
      "Epoch [1/10], Step [599/899], Loss: 0.0117\n",
      "Epoch [1/10], Step [600/899], Loss: 0.0080\n",
      "Epoch [1/10], Step [601/899], Loss: 0.0097\n",
      "Epoch [1/10], Step [602/899], Loss: 0.0072\n",
      "Epoch [1/10], Step [603/899], Loss: 0.0052\n",
      "Epoch [1/10], Step [604/899], Loss: 0.0032\n",
      "Epoch [1/10], Step [605/899], Loss: 0.0030\n",
      "Epoch [1/10], Step [606/899], Loss: 0.0028\n",
      "Epoch [1/10], Step [607/899], Loss: 0.0034\n",
      "Epoch [1/10], Step [608/899], Loss: 0.0031\n",
      "Epoch [1/10], Step [609/899], Loss: 0.0029\n",
      "Epoch [1/10], Step [610/899], Loss: 0.0030\n",
      "Epoch [1/10], Step [611/899], Loss: 0.0081\n",
      "Epoch [1/10], Step [612/899], Loss: 0.0065\n",
      "processed/train/19/T37UCR_20180621T083651centinel2_nvdi_labels.npy 0.55507505\n",
      "(2304, 228, 228, 3)\n",
      "Epoch [1/10], Step [613/899], Loss: 0.0029\n",
      "Epoch [1/10], Step [614/899], Loss: 0.0091\n",
      "Epoch [1/10], Step [615/899], Loss: 0.0078\n",
      "Epoch [1/10], Step [616/899], Loss: 0.0071\n",
      "Epoch [1/10], Step [617/899], Loss: 0.0031\n",
      "Epoch [1/10], Step [618/899], Loss: 0.0029\n",
      "Epoch [1/10], Step [619/899], Loss: 0.0019\n",
      "Epoch [1/10], Step [620/899], Loss: 0.0099\n",
      "Epoch [1/10], Step [621/899], Loss: 0.0024\n",
      "Epoch [1/10], Step [622/899], Loss: 0.0034\n",
      "Epoch [1/10], Step [623/899], Loss: 0.0023\n",
      "Epoch [1/10], Step [624/899], Loss: 0.0020\n",
      "Epoch [1/10], Step [625/899], Loss: 0.0024\n",
      "Epoch [1/10], Step [626/899], Loss: 0.0019\n",
      "Epoch [1/10], Step [627/899], Loss: 0.0014\n",
      "Epoch [1/10], Step [628/899], Loss: 0.0020\n",
      "Epoch [1/10], Step [629/899], Loss: 0.0032\n",
      "Epoch [1/10], Step [630/899], Loss: 0.0030\n",
      "Epoch [1/10], Step [631/899], Loss: 0.0096\n",
      "Epoch [1/10], Step [632/899], Loss: 0.0133\n",
      "Epoch [1/10], Step [633/899], Loss: 0.0073\n",
      "Epoch [1/10], Step [634/899], Loss: 0.0063\n",
      "Epoch [1/10], Step [635/899], Loss: 0.0117\n",
      "Epoch [1/10], Step [636/899], Loss: 0.0034\n",
      "Epoch [1/10], Step [637/899], Loss: 0.0035\n",
      "Epoch [1/10], Step [638/899], Loss: 0.0038\n",
      "Epoch [1/10], Step [639/899], Loss: 0.0016\n",
      "Epoch [1/10], Step [640/899], Loss: 0.0023\n",
      "Epoch [1/10], Step [641/899], Loss: 0.0017\n",
      "Epoch [1/10], Step [642/899], Loss: 0.0016\n",
      "Epoch [1/10], Step [643/899], Loss: 0.0023\n",
      "Epoch [1/10], Step [644/899], Loss: 0.0013\n",
      "Epoch [1/10], Step [645/899], Loss: 0.0010\n",
      "Epoch [1/10], Step [646/899], Loss: 0.0021\n",
      "Epoch [1/10], Step [647/899], Loss: 0.0142\n",
      "Epoch [1/10], Step [648/899], Loss: 0.0110\n",
      "processed/train/20/T37UCR_20180810T083601centinel2_nvdi_labels.npy 0.51132125\n",
      "(2304, 228, 228, 3)\n",
      "Epoch [1/10], Step [649/899], Loss: 0.0046\n",
      "Epoch [1/10], Step [650/899], Loss: 0.0094\n",
      "Epoch [1/10], Step [651/899], Loss: 0.0055\n",
      "Epoch [1/10], Step [652/899], Loss: 0.0070\n",
      "Epoch [1/10], Step [653/899], Loss: 0.0026\n",
      "Epoch [1/10], Step [654/899], Loss: 0.0013\n",
      "Epoch [1/10], Step [655/899], Loss: 0.0017\n",
      "Epoch [1/10], Step [656/899], Loss: 0.0024\n",
      "Epoch [1/10], Step [657/899], Loss: 0.0011\n",
      "Epoch [1/10], Step [658/899], Loss: 0.0020\n",
      "Epoch [1/10], Step [659/899], Loss: 0.0016\n",
      "Epoch [1/10], Step [660/899], Loss: 0.0020\n",
      "Epoch [1/10], Step [661/899], Loss: 0.0020\n",
      "Epoch [1/10], Step [662/899], Loss: 0.0021\n",
      "Epoch [1/10], Step [663/899], Loss: 0.0010\n",
      "Epoch [1/10], Step [664/899], Loss: 0.0023\n",
      "Epoch [1/10], Step [665/899], Loss: 0.0042\n",
      "Epoch [1/10], Step [666/899], Loss: 0.0026\n",
      "Epoch [1/10], Step [667/899], Loss: 0.0054\n",
      "Epoch [1/10], Step [668/899], Loss: 0.0149\n",
      "Epoch [1/10], Step [669/899], Loss: 0.0067\n",
      "Epoch [1/10], Step [670/899], Loss: 0.0091\n",
      "Epoch [1/10], Step [671/899], Loss: 0.0158\n",
      "Epoch [1/10], Step [672/899], Loss: 0.0057\n",
      "Epoch [1/10], Step [673/899], Loss: 0.0041\n",
      "Epoch [1/10], Step [674/899], Loss: 0.0029\n",
      "Epoch [1/10], Step [675/899], Loss: 0.0018\n",
      "Epoch [1/10], Step [676/899], Loss: 0.0065\n",
      "Epoch [1/10], Step [677/899], Loss: 0.0020\n",
      "Epoch [1/10], Step [678/899], Loss: 0.0019\n",
      "Epoch [1/10], Step [679/899], Loss: 0.0051\n",
      "Epoch [1/10], Step [680/899], Loss: 0.0028\n",
      "Epoch [1/10], Step [681/899], Loss: 0.0014\n",
      "Epoch [1/10], Step [682/899], Loss: 0.0024\n",
      "Epoch [1/10], Step [683/899], Loss: 0.0121\n",
      "Epoch [1/10], Step [684/899], Loss: 0.0066\n",
      "processed/train/21/T37UCR_20180820T083601centinel2_nvdi_labels.npy 0.42861512\n",
      "(2304, 228, 228, 3)\n",
      "Epoch [1/10], Step [685/899], Loss: 0.0042\n",
      "Epoch [1/10], Step [686/899], Loss: 0.0082\n",
      "Epoch [1/10], Step [687/899], Loss: 0.0041\n",
      "Epoch [1/10], Step [688/899], Loss: 0.0049\n",
      "Epoch [1/10], Step [689/899], Loss: 0.0026\n",
      "Epoch [1/10], Step [690/899], Loss: 0.0014\n",
      "Epoch [1/10], Step [691/899], Loss: 0.0014\n",
      "Epoch [1/10], Step [692/899], Loss: 0.0012\n",
      "Epoch [1/10], Step [693/899], Loss: 0.0014\n",
      "Epoch [1/10], Step [694/899], Loss: 0.0028\n",
      "Epoch [1/10], Step [695/899], Loss: 0.0022\n",
      "Epoch [1/10], Step [696/899], Loss: 0.0010\n",
      "Epoch [1/10], Step [697/899], Loss: 0.0014\n",
      "Epoch [1/10], Step [698/899], Loss: 0.0019\n",
      "Epoch [1/10], Step [699/899], Loss: 0.0012\n",
      "Epoch [1/10], Step [700/899], Loss: 0.0023\n",
      "Epoch [1/10], Step [701/899], Loss: 0.0040\n",
      "Epoch [1/10], Step [702/899], Loss: 0.0027\n",
      "Epoch [1/10], Step [703/899], Loss: 0.0065\n",
      "Epoch [1/10], Step [704/899], Loss: 0.0100\n",
      "Epoch [1/10], Step [705/899], Loss: 0.0041\n",
      "Epoch [1/10], Step [706/899], Loss: 0.0052\n",
      "Epoch [1/10], Step [707/899], Loss: 0.0155\n",
      "Epoch [1/10], Step [708/899], Loss: 0.0070\n",
      "Epoch [1/10], Step [709/899], Loss: 0.0047\n",
      "Epoch [1/10], Step [710/899], Loss: 0.0031\n",
      "Epoch [1/10], Step [711/899], Loss: 0.0018\n",
      "Epoch [1/10], Step [712/899], Loss: 0.0051\n",
      "Epoch [1/10], Step [713/899], Loss: 0.0032\n",
      "Epoch [1/10], Step [714/899], Loss: 0.0025\n",
      "Epoch [1/10], Step [715/899], Loss: 0.0070\n",
      "Epoch [1/10], Step [716/899], Loss: 0.0063\n",
      "Epoch [1/10], Step [717/899], Loss: 0.0030\n",
      "Epoch [1/10], Step [718/899], Loss: 0.0038\n",
      "Epoch [1/10], Step [719/899], Loss: 0.0086\n",
      "Epoch [1/10], Step [720/899], Loss: 0.0051\n",
      "processed/train/22/T37UCR_20180825T083549centinel2_nvdi_labels.npy 0.40783975\n",
      "(2304, 228, 228, 3)\n",
      "Epoch [1/10], Step [721/899], Loss: 0.0120\n",
      "Epoch [1/10], Step [722/899], Loss: 0.0194\n",
      "Epoch [1/10], Step [723/899], Loss: 0.0093\n",
      "Epoch [1/10], Step [724/899], Loss: 0.0076\n",
      "Epoch [1/10], Step [725/899], Loss: 0.0047\n",
      "Epoch [1/10], Step [726/899], Loss: 0.0028\n",
      "Epoch [1/10], Step [727/899], Loss: 0.0016\n",
      "Epoch [1/10], Step [728/899], Loss: 0.0046\n",
      "Epoch [1/10], Step [729/899], Loss: 0.0015\n",
      "Epoch [1/10], Step [730/899], Loss: 0.0013\n",
      "Epoch [1/10], Step [731/899], Loss: 0.0011\n",
      "Epoch [1/10], Step [732/899], Loss: 0.0024\n",
      "Epoch [1/10], Step [733/899], Loss: 0.0031\n",
      "Epoch [1/10], Step [734/899], Loss: 0.0011\n",
      "Epoch [1/10], Step [735/899], Loss: 0.0011\n",
      "Epoch [1/10], Step [736/899], Loss: 0.0013\n",
      "Epoch [1/10], Step [737/899], Loss: 0.0038\n",
      "Epoch [1/10], Step [738/899], Loss: 0.0026\n",
      "Epoch [1/10], Step [739/899], Loss: 0.0075\n",
      "Epoch [1/10], Step [740/899], Loss: 0.0078\n",
      "Epoch [1/10], Step [741/899], Loss: 0.0037\n",
      "Epoch [1/10], Step [742/899], Loss: 0.0061\n",
      "Epoch [1/10], Step [743/899], Loss: 0.0102\n",
      "Epoch [1/10], Step [744/899], Loss: 0.0055\n",
      "Epoch [1/10], Step [745/899], Loss: 0.0052\n",
      "Epoch [1/10], Step [746/899], Loss: 0.0027\n",
      "Epoch [1/10], Step [747/899], Loss: 0.0014\n",
      "Epoch [1/10], Step [748/899], Loss: 0.0049\n",
      "Epoch [1/10], Step [749/899], Loss: 0.0045\n",
      "Epoch [1/10], Step [750/899], Loss: 0.0027\n",
      "Epoch [1/10], Step [751/899], Loss: 0.0083\n",
      "Epoch [1/10], Step [752/899], Loss: 0.0097\n",
      "Epoch [1/10], Step [753/899], Loss: 0.0030\n",
      "Epoch [1/10], Step [754/899], Loss: 0.0028\n",
      "Epoch [1/10], Step [755/899], Loss: 0.0112\n",
      "Epoch [1/10], Step [756/899], Loss: 0.0057\n",
      "processed/train/23/T37UCS_20180502T083601centinel2_nvdi_labels.npy 0.31234613\n",
      "(2304, 228, 228, 3)\n",
      "Epoch [1/10], Step [757/899], Loss: 0.0211\n",
      "Epoch [1/10], Step [758/899], Loss: 0.0119\n",
      "Epoch [1/10], Step [759/899], Loss: 0.0100\n",
      "Epoch [1/10], Step [760/899], Loss: 0.0029\n",
      "Epoch [1/10], Step [761/899], Loss: 0.0126\n",
      "Epoch [1/10], Step [762/899], Loss: 0.0055\n",
      "Epoch [1/10], Step [763/899], Loss: 0.0042\n",
      "Epoch [1/10], Step [764/899], Loss: 0.0043\n",
      "Epoch [1/10], Step [765/899], Loss: 0.0085\n",
      "Epoch [1/10], Step [766/899], Loss: 0.0013\n",
      "Epoch [1/10], Step [767/899], Loss: 0.0025\n",
      "Epoch [1/10], Step [768/899], Loss: 0.0014\n",
      "Epoch [1/10], Step [769/899], Loss: 0.0013\n",
      "Epoch [1/10], Step [770/899], Loss: 0.0012\n",
      "Epoch [1/10], Step [771/899], Loss: 0.0011\n",
      "Epoch [1/10], Step [772/899], Loss: 0.0016\n",
      "Epoch [1/10], Step [773/899], Loss: 0.0009\n",
      "Epoch [1/10], Step [774/899], Loss: 0.0021\n",
      "Epoch [1/10], Step [775/899], Loss: 0.0005\n",
      "Epoch [1/10], Step [776/899], Loss: 0.0011\n",
      "Epoch [1/10], Step [777/899], Loss: 0.0037\n",
      "Epoch [1/10], Step [778/899], Loss: 0.0068\n",
      "Epoch [1/10], Step [779/899], Loss: 0.0023\n",
      "Epoch [1/10], Step [780/899], Loss: 0.0026\n",
      "Epoch [1/10], Step [781/899], Loss: 0.0045\n",
      "Epoch [1/10], Step [782/899], Loss: 0.0063\n",
      "Epoch [1/10], Step [783/899], Loss: 0.0051\n",
      "Epoch [1/10], Step [784/899], Loss: 0.0044\n",
      "Epoch [1/10], Step [785/899], Loss: 0.0043\n",
      "Epoch [1/10], Step [786/899], Loss: 0.0032\n",
      "Epoch [1/10], Step [787/899], Loss: 0.0039\n",
      "Epoch [1/10], Step [788/899], Loss: 0.0094\n",
      "Epoch [1/10], Step [789/899], Loss: 0.0024\n",
      "Epoch [1/10], Step [790/899], Loss: 0.0052\n",
      "Epoch [1/10], Step [791/899], Loss: 0.0101\n",
      "Epoch [1/10], Step [792/899], Loss: 0.0036\n",
      "processed/train/24/T37UCS_20180726T084009centinel2_nvdi_labels.npy 0.56238866\n",
      "(2304, 228, 228, 3)\n",
      "Epoch [1/10], Step [793/899], Loss: 0.0924\n",
      "Epoch [1/10], Step [794/899], Loss: 0.0544\n",
      "Epoch [1/10], Step [795/899], Loss: 0.0504\n",
      "Epoch [1/10], Step [796/899], Loss: 0.0334\n",
      "Epoch [1/10], Step [797/899], Loss: 0.0576\n",
      "Epoch [1/10], Step [798/899], Loss: 0.0460\n",
      "Epoch [1/10], Step [799/899], Loss: 0.0331\n",
      "Epoch [1/10], Step [800/899], Loss: 0.0235\n",
      "Epoch [1/10], Step [801/899], Loss: 0.0193\n",
      "Epoch [1/10], Step [802/899], Loss: 0.0192\n",
      "Epoch [1/10], Step [803/899], Loss: 0.0186\n",
      "Epoch [1/10], Step [804/899], Loss: 0.0164\n",
      "Epoch [1/10], Step [805/899], Loss: 0.0076\n",
      "Epoch [1/10], Step [806/899], Loss: 0.0054\n",
      "Epoch [1/10], Step [807/899], Loss: 0.0029\n",
      "Epoch [1/10], Step [808/899], Loss: 0.0030\n",
      "Epoch [1/10], Step [809/899], Loss: 0.0019\n",
      "Epoch [1/10], Step [810/899], Loss: 0.0043\n",
      "Epoch [1/10], Step [811/899], Loss: 0.0054\n",
      "Epoch [1/10], Step [812/899], Loss: 0.0048\n",
      "Epoch [1/10], Step [813/899], Loss: 0.0080\n",
      "Epoch [1/10], Step [814/899], Loss: 0.0092\n",
      "Epoch [1/10], Step [815/899], Loss: 0.0060\n",
      "Epoch [1/10], Step [816/899], Loss: 0.0098\n",
      "Epoch [1/10], Step [817/899], Loss: 0.0051\n",
      "Epoch [1/10], Step [818/899], Loss: 0.0032\n",
      "Epoch [1/10], Step [819/899], Loss: 0.0029\n",
      "Epoch [1/10], Step [820/899], Loss: 0.0024\n",
      "Epoch [1/10], Step [821/899], Loss: 0.0018\n",
      "Epoch [1/10], Step [822/899], Loss: 0.0040\n",
      "Epoch [1/10], Step [823/899], Loss: 0.0031\n",
      "Epoch [1/10], Step [824/899], Loss: 0.0018\n",
      "Epoch [1/10], Step [825/899], Loss: 0.0022\n",
      "Epoch [1/10], Step [826/899], Loss: 0.0025\n",
      "Epoch [1/10], Step [827/899], Loss: 0.0025\n",
      "Epoch [1/10], Step [828/899], Loss: 0.0062\n",
      "processed/train/25/T37UCS_20180810T083601centinel2_nvdi_labels.npy 0.574699\n",
      "(2304, 228, 228, 3)\n",
      "Epoch [1/10], Step [829/899], Loss: 0.0023\n",
      "Epoch [1/10], Step [830/899], Loss: 0.0021\n",
      "Epoch [1/10], Step [831/899], Loss: 0.0009\n",
      "Epoch [1/10], Step [832/899], Loss: 0.0037\n",
      "Epoch [1/10], Step [833/899], Loss: 0.0025\n",
      "Epoch [1/10], Step [834/899], Loss: 0.0026\n",
      "Epoch [1/10], Step [835/899], Loss: 0.0018\n",
      "Epoch [1/10], Step [836/899], Loss: 0.0012\n",
      "Epoch [1/10], Step [837/899], Loss: 0.0103\n",
      "Epoch [1/10], Step [838/899], Loss: 0.0019\n",
      "Epoch [1/10], Step [839/899], Loss: 0.0028\n",
      "Epoch [1/10], Step [840/899], Loss: 0.0015\n",
      "Epoch [1/10], Step [841/899], Loss: 0.0015\n",
      "Epoch [1/10], Step [842/899], Loss: 0.0013\n",
      "Epoch [1/10], Step [843/899], Loss: 0.0012\n",
      "Epoch [1/10], Step [844/899], Loss: 0.0024\n",
      "Epoch [1/10], Step [845/899], Loss: 0.0014\n",
      "Epoch [1/10], Step [846/899], Loss: 0.0017\n",
      "Epoch [1/10], Step [847/899], Loss: 0.0020\n",
      "Epoch [1/10], Step [848/899], Loss: 0.0062\n",
      "Epoch [1/10], Step [849/899], Loss: 0.0017\n",
      "Epoch [1/10], Step [850/899], Loss: 0.0022\n",
      "Epoch [1/10], Step [851/899], Loss: 0.0013\n",
      "Epoch [1/10], Step [852/899], Loss: 0.0023\n",
      "Epoch [1/10], Step [853/899], Loss: 0.0015\n",
      "Epoch [1/10], Step [854/899], Loss: 0.0017\n",
      "Epoch [1/10], Step [855/899], Loss: 0.0013\n",
      "Epoch [1/10], Step [856/899], Loss: 0.0012\n",
      "Epoch [1/10], Step [857/899], Loss: 0.0028\n",
      "Epoch [1/10], Step [858/899], Loss: 0.0028\n",
      "Epoch [1/10], Step [859/899], Loss: 0.0034\n",
      "Epoch [1/10], Step [860/899], Loss: 0.0016\n",
      "Epoch [1/10], Step [861/899], Loss: 0.0026\n",
      "Epoch [1/10], Step [862/899], Loss: 0.0025\n",
      "Epoch [1/10], Step [863/899], Loss: 0.0024\n",
      "Epoch [1/10], Step [864/899], Loss: 0.0068\n",
      "processed/train/26/T37UCS_20180820T083601centinel2_nvdi_labels.npy 0.5171251\n",
      "(2304, 228, 228, 3)\n",
      "Epoch [1/10], Step [865/899], Loss: 0.0059\n",
      "Epoch [1/10], Step [866/899], Loss: 0.0126\n",
      "Epoch [1/10], Step [867/899], Loss: 0.0125\n",
      "Epoch [1/10], Step [868/899], Loss: 0.0174\n",
      "Epoch [1/10], Step [869/899], Loss: 0.0034\n",
      "Epoch [1/10], Step [870/899], Loss: 0.0033\n",
      "Epoch [1/10], Step [871/899], Loss: 0.0034\n",
      "Epoch [1/10], Step [872/899], Loss: 0.0033\n",
      "Epoch [1/10], Step [873/899], Loss: 0.0139\n",
      "Epoch [1/10], Step [874/899], Loss: 0.0015\n",
      "Epoch [1/10], Step [875/899], Loss: 0.0012\n",
      "Epoch [1/10], Step [876/899], Loss: 0.0009\n",
      "Epoch [1/10], Step [877/899], Loss: 0.0010\n",
      "Epoch [1/10], Step [878/899], Loss: 0.0015\n",
      "Epoch [1/10], Step [879/899], Loss: 0.0022\n",
      "Epoch [1/10], Step [880/899], Loss: 0.0048\n",
      "Epoch [1/10], Step [881/899], Loss: 0.0038\n",
      "Epoch [1/10], Step [882/899], Loss: 0.0048\n",
      "Epoch [1/10], Step [883/899], Loss: 0.0059\n",
      "Epoch [1/10], Step [884/899], Loss: 0.0130\n",
      "Epoch [1/10], Step [885/899], Loss: 0.0060\n",
      "Epoch [1/10], Step [886/899], Loss: 0.0058\n",
      "Epoch [1/10], Step [887/899], Loss: 0.0055\n",
      "Epoch [1/10], Step [888/899], Loss: 0.0044\n",
      "Epoch [1/10], Step [889/899], Loss: 0.0055\n",
      "Epoch [1/10], Step [890/899], Loss: 0.0049\n",
      "Epoch [1/10], Step [891/899], Loss: 0.0034\n",
      "Epoch [1/10], Step [892/899], Loss: 0.0026\n",
      "Epoch [1/10], Step [893/899], Loss: 0.0050\n",
      "Epoch [1/10], Step [894/899], Loss: 0.0019\n",
      "Epoch [1/10], Step [895/899], Loss: 0.0011\n",
      "Epoch [1/10], Step [896/899], Loss: 0.0011\n",
      "Epoch [1/10], Step [897/899], Loss: 0.0017\n",
      "Epoch [1/10], Step [898/899], Loss: 0.0018\n",
      "Epoch [1/10], Step [899/899], Loss: 0.0017\n",
      "Epoch [1/10], Step [900/899], Loss: 0.0094\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_549931/4160940196.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;31m# Move tensors to the configured device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mended\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_typing.py\u001b[0m in \u001b[0;36mwrap_generator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    374\u001b[0m                     \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_549931/1235904957.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_549931/1235904957.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan_to_num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mposinf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneginf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "\n",
    "total_step = 899#len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, labels) in enumerate(dataloader):  \n",
    "        # Move tensors to the configured device\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        loss = train_loss(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                       .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "            \n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        mse = []\n",
    "        \n",
    "        for data, labels in valloader:\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(data)\n",
    "            predicted = outputs.data\n",
    "            mse.append(float(train_loss(labels.data,predicted))) \n",
    "            del data, labels, outputs\n",
    "    \n",
    "        print('MSE of the network on the {} validation samples: {} %'.format(5000, numpy.asarray(mse).mean())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73ff6564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed/validate/0/T36UXA_20180611T083601centinel2_nvdi_labels.npy 0.5701243\n",
      "processed/validate/1/T37UCR_20180731T083601centinel2_nvdi_labels.npy 0.54268944\n",
      "processed/validate/2/T37UCR_20180904T083549centinel2_nvdi_labels.npy 0.33022287\n",
      "processed/validate/3/T37UCS_20180825T083549centinel2_nvdi_labels.npy 0.4912743\n",
      "MSE of the network on the 5000 test samples: 0.019864353941456 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    mse = []\n",
    "\n",
    "    for data, labels in valloader:\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(data)\n",
    "        predicted = outputs.data\n",
    "        mse.append(float(train_loss(labels.data,predicted))) \n",
    "        del data, labels, outputs\n",
    "\n",
    "    print('MSE of the network on the {} test samples: {} %'.format(5000, numpy.asarray(mse).mean())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71d19e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed/spring/0/T36UXA_20180502T083601centinel2_nvdi_labels.npy 0.33934337\n",
      "(2304, 228, 228, 3)\n",
      "processed/spring/1/T36UXA_20180601T083651centinel2_nvdi_labels.npy 0.53730196\n",
      "(2304, 228, 228, 3)\n",
      "MSE of the network on the 5000 test samples: 0.032713217524966844 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    mse = []\n",
    "\n",
    "    for data, labels in valloader:\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(data)\n",
    "        predicted = outputs.data\n",
    "        mse.append(float(train_loss(labels.data,predicted))) \n",
    "        del data, labels, outputs\n",
    "\n",
    "    print('MSE of the network on the {} test samples: {} %'.format(5000, numpy.asarray(mse).mean())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "59f77705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed/outumn/2/T36UXA_20180825T083549centinel2_nvdi_labels.npy 0.44718567\n",
      "(2304, 228, 228, 3)\n",
      "processed/outumn/3/T36UXA_20180904T083549centinel2_nvdi_labels.npy 0.34236798\n",
      "(2304, 228, 228, 3)\n",
      "MSE of the network on the 5000 test samples: 0.03517451195511967 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    mse = []\n",
    "\n",
    "    for data, labels in valloader:\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(data)\n",
    "        predicted = outputs.data\n",
    "        mse.append(float(train_loss(labels.data,predicted))) \n",
    "        del data, labels, outputs\n",
    "\n",
    "    print('MSE of the network on the {} test samples: {} %'.format(5000, numpy.asarray(mse).mean())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0fb91ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed/train/0/T36UXA_20180621T083651centinel2_nvdi_labels.npy 0.6065357\n",
      "(2304, 228, 228, 3)\n",
      "[tensor([[[[ 830.,  890.,  842.,  ...,  906.,  932.,  956.],\n",
      "          [ 846.,  856.,  846.,  ...,  946.,  924.,  966.],\n",
      "          [ 854.,  842.,  866.,  ...,  964.,  928.,  942.],\n",
      "          ...,\n",
      "          [1016., 1010., 1096.,  ..., 1146., 1158., 1226.],\n",
      "          [1046., 1002., 1056.,  ..., 1150., 1194., 1274.],\n",
      "          [1084., 1042., 1076.,  ..., 1230., 1368., 1426.]],\n",
      "\n",
      "         [[1380., 1430., 1386.,  ..., 1516., 1544., 1600.],\n",
      "          [1386., 1408., 1392.,  ..., 1554., 1540., 1598.],\n",
      "          [1396., 1376., 1436.,  ..., 1574., 1544., 1538.],\n",
      "          ...,\n",
      "          [1632., 1706., 1774.,  ..., 1698., 1678., 1686.],\n",
      "          [1652., 1686., 1742.,  ..., 1680., 1682., 1686.],\n",
      "          [1682., 1670., 1762.,  ..., 1696., 1776., 1748.]],\n",
      "\n",
      "         [[1688., 1686., 1682.,  ..., 1754., 1776., 1826.],\n",
      "          [1680., 1676., 1694.,  ..., 1772., 1732., 1768.],\n",
      "          [1684., 1694., 1700.,  ..., 1776., 1726., 1752.],\n",
      "          ...,\n",
      "          [1742., 1758., 1804.,  ..., 1864., 1868., 1904.],\n",
      "          [1760., 1738., 1770.,  ..., 1868., 1894., 1898.],\n",
      "          [1774., 1776., 1812.,  ..., 1850., 1980., 1970.]]],\n",
      "\n",
      "\n",
      "        [[[1262., 1138., 1044.,  ..., 1238., 1312., 1146.],\n",
      "          [1252., 1124., 1024.,  ..., 1082., 1112., 1022.],\n",
      "          [1016., 1060.,  954.,  ..., 1072.,  986.,  950.],\n",
      "          ...,\n",
      "          [ 874.,  906.,  864.,  ..., 1490., 1444., 1704.],\n",
      "          [ 964., 1018.,  910.,  ..., 1736., 1372., 1228.],\n",
      "          [1000., 1034.,  890.,  ..., 2014., 1516., 1432.]],\n",
      "\n",
      "         [[1782., 1706., 1654.,  ..., 1658., 1754., 1656.],\n",
      "          [1742., 1666., 1518.,  ..., 1606., 1646., 1632.],\n",
      "          [1506., 1600., 1504.,  ..., 1620., 1544., 1540.],\n",
      "          ...,\n",
      "          [1474., 1646., 1710.,  ..., 1854., 1860., 2036.],\n",
      "          [1544., 1688., 1652.,  ..., 2050., 1878., 1864.],\n",
      "          [1486., 1520., 1420.,  ..., 2294., 2080., 2022.]],\n",
      "\n",
      "         [[1896., 1816., 1792.,  ..., 1830., 1910., 1822.],\n",
      "          [1924., 1806., 1730.,  ..., 1804., 1812., 1762.],\n",
      "          [1736., 1768., 1708.,  ..., 1792., 1750., 1744.],\n",
      "          ...,\n",
      "          [1690., 1762., 1760.,  ..., 1966., 1964., 2134.],\n",
      "          [1726., 1782., 1712.,  ..., 2098., 1942., 1884.],\n",
      "          [1784., 1798., 1678.,  ..., 2250., 2042., 2004.]]],\n",
      "\n",
      "\n",
      "        [[[1206., 1146.,  976.,  ..., 2132., 1914., 1484.],\n",
      "          [1558., 1504., 1322.,  ..., 2110., 2254., 1678.],\n",
      "          [1420., 1514., 1282.,  ..., 2160., 2328., 1978.],\n",
      "          ...,\n",
      "          [1696., 1632., 1884.,  ..., 1348., 1406., 1412.],\n",
      "          [1598., 1704., 2198.,  ..., 1386., 1402., 1420.],\n",
      "          [1390., 1788., 2492.,  ..., 1392., 1384., 1394.]],\n",
      "\n",
      "         [[1574., 1496., 1464.,  ..., 2358., 2220., 1862.],\n",
      "          [1632., 1636., 1604.,  ..., 2338., 2340., 1944.],\n",
      "          [1538., 1626., 1526.,  ..., 2378., 2386., 2264.],\n",
      "          ...,\n",
      "          [2052., 2094., 2182.,  ..., 1868., 1896., 1902.],\n",
      "          [2028., 2082., 2238.,  ..., 1880., 1882., 1910.],\n",
      "          [1968., 2110., 2408.,  ..., 1922., 1908., 1920.]],\n",
      "\n",
      "         [[1772., 1756., 1756.,  ..., 2324., 2218., 1984.],\n",
      "          [1886., 1870., 1852.,  ..., 2316., 2404., 2070.],\n",
      "          [1842., 1898., 1840.,  ..., 2318., 2416., 2274.],\n",
      "          ...,\n",
      "          [2146., 2090., 2208.,  ..., 1994., 1946., 1992.],\n",
      "          [2098., 2098., 2340.,  ..., 2024., 1994., 2020.],\n",
      "          [2018., 2160., 2438.,  ..., 1976., 1980., 1992.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[1122., 1536., 2122.,  ..., 1718., 1718., 1708.],\n",
      "          [1184., 1916., 2304.,  ..., 1682., 1688., 1726.],\n",
      "          [1434., 2158., 2332.,  ..., 1592., 1628., 1688.],\n",
      "          ...,\n",
      "          [1460., 1436., 1406.,  ..., 1612., 1574., 1556.],\n",
      "          [1512., 1420., 1402.,  ..., 1604., 1604., 1562.],\n",
      "          [1526., 1436., 1462.,  ..., 1572., 1640., 1566.]],\n",
      "\n",
      "         [[1750., 1862., 2098.,  ..., 1822., 1842., 1874.],\n",
      "          [1692., 1978., 2170.,  ..., 1814., 1812., 1874.],\n",
      "          [1814., 2102., 2176.,  ..., 1786., 1766., 1860.],\n",
      "          ...,\n",
      "          [1798., 1802., 1780.,  ..., 1778., 1792., 1758.],\n",
      "          [1804., 1800., 1790.,  ..., 1784., 1768., 1774.],\n",
      "          [1828., 1778., 1794.,  ..., 1802., 1792., 1770.]],\n",
      "\n",
      "         [[1782., 2008., 2280.,  ..., 2070., 2094., 2120.],\n",
      "          [1822., 2164., 2352.,  ..., 2054., 2092., 2108.],\n",
      "          [1916., 2266., 2360.,  ..., 2016., 2040., 2062.],\n",
      "          ...,\n",
      "          [1970., 1946., 1932.,  ..., 2058., 2012., 1988.],\n",
      "          [1958., 1938., 1942.,  ..., 2038., 2030., 2004.],\n",
      "          [1940., 1922., 1950.,  ..., 2040., 2034., 2008.]]],\n",
      "\n",
      "\n",
      "        [[[1508., 1466., 1484.,  ..., 1630., 1618., 1568.],\n",
      "          [1462., 1476., 1462.,  ..., 1610., 1584., 1548.],\n",
      "          [1460., 1478., 1448.,  ..., 1582., 1572., 1520.],\n",
      "          ...,\n",
      "          [1030.,  862.,  766.,  ..., 1300., 1352., 1436.],\n",
      "          [1136.,  994.,  898.,  ..., 1344., 1526., 1618.],\n",
      "          [1342., 1134.,  984.,  ..., 1348., 1538., 1642.]],\n",
      "\n",
      "         [[1820., 1800., 1808.,  ..., 1804., 1790., 1740.],\n",
      "          [1802., 1788., 1782.,  ..., 1790., 1760., 1744.],\n",
      "          [1790., 1824., 1802.,  ..., 1786., 1790., 1754.],\n",
      "          ...,\n",
      "          [1766., 1642., 1488.,  ..., 1786., 1828., 1898.],\n",
      "          [1834., 1748., 1640.,  ..., 1858., 1924., 1956.],\n",
      "          [1928., 1828., 1726.,  ..., 1870., 1938., 1972.]],\n",
      "\n",
      "         [[1934., 1952., 1960.,  ..., 2052., 2032., 1974.],\n",
      "          [1936., 1946., 1952.,  ..., 2036., 2004., 1988.],\n",
      "          [1932., 1966., 1982.,  ..., 2010., 1980., 1996.],\n",
      "          ...,\n",
      "          [1776., 1678., 1660.,  ..., 1904., 1960., 1988.],\n",
      "          [1818., 1756., 1724.,  ..., 1936., 2008., 2056.],\n",
      "          [1918., 1826., 1752.,  ..., 1924., 2038., 2070.]]],\n",
      "\n",
      "\n",
      "        [[[1516., 1284., 1022.,  ..., 1480., 1720., 1768.],\n",
      "          [1446., 1532., 1206.,  ..., 1494., 1798., 1760.],\n",
      "          [1316., 1622., 1490.,  ..., 1210., 1564., 1564.],\n",
      "          ...,\n",
      "          [1630., 1550., 1386.,  ..., 1070., 1080., 1136.],\n",
      "          [1344., 1166., 1230.,  ..., 1068., 1120., 1140.],\n",
      "          [1428., 1442., 1582.,  ..., 1038., 1096., 1090.]],\n",
      "\n",
      "         [[2074., 1926., 1790.,  ..., 1846., 1928., 1940.],\n",
      "          [2030., 2046., 1928.,  ..., 1890., 1930., 1926.],\n",
      "          [1978., 2098., 2082.,  ..., 1726., 1872., 1856.],\n",
      "          ...,\n",
      "          [2026., 2022., 2054.,  ..., 1716., 1688., 1750.],\n",
      "          [1962., 1920., 1928.,  ..., 1676., 1700., 1758.],\n",
      "          [1898., 2018., 2054.,  ..., 1668., 1694., 1732.]],\n",
      "\n",
      "         [[1998., 1892., 1754.,  ..., 1988., 2098., 2100.],\n",
      "          [1928., 1978., 1846.,  ..., 1990., 2106., 2138.],\n",
      "          [1888., 1992., 1984.,  ..., 1832., 2024., 2024.],\n",
      "          ...,\n",
      "          [2098., 2098., 2030.,  ..., 1784., 1778., 1814.],\n",
      "          [2000., 1960., 1934.,  ..., 1808., 1768., 1804.],\n",
      "          [1902., 1936., 2042.,  ..., 1778., 1786., 1802.]]]]), tensor([[[0.6786]],\n",
      "\n",
      "        [[0.6674]],\n",
      "\n",
      "        [[0.4695]],\n",
      "\n",
      "        [[0.4527]],\n",
      "\n",
      "        [[0.6363]],\n",
      "\n",
      "        [[0.7127]],\n",
      "\n",
      "        [[0.7376]],\n",
      "\n",
      "        [[0.6913]],\n",
      "\n",
      "        [[0.6853]],\n",
      "\n",
      "        [[0.6896]],\n",
      "\n",
      "        [[0.6396]],\n",
      "\n",
      "        [[0.6147]],\n",
      "\n",
      "        [[0.6399]],\n",
      "\n",
      "        [[0.6238]],\n",
      "\n",
      "        [[0.5887]],\n",
      "\n",
      "        [[0.6294]],\n",
      "\n",
      "        [[0.6425]],\n",
      "\n",
      "        [[0.7047]],\n",
      "\n",
      "        [[0.6759]],\n",
      "\n",
      "        [[0.7020]],\n",
      "\n",
      "        [[0.6628]],\n",
      "\n",
      "        [[0.7276]],\n",
      "\n",
      "        [[0.6529]],\n",
      "\n",
      "        [[0.6649]],\n",
      "\n",
      "        [[0.7356]],\n",
      "\n",
      "        [[0.6374]],\n",
      "\n",
      "        [[0.6318]],\n",
      "\n",
      "        [[0.5066]],\n",
      "\n",
      "        [[0.5966]],\n",
      "\n",
      "        [[0.6244]],\n",
      "\n",
      "        [[0.7481]],\n",
      "\n",
      "        [[0.7066]],\n",
      "\n",
      "        [[0.7435]],\n",
      "\n",
      "        [[0.7237]],\n",
      "\n",
      "        [[0.6426]],\n",
      "\n",
      "        [[0.6039]],\n",
      "\n",
      "        [[0.7096]],\n",
      "\n",
      "        [[0.7112]],\n",
      "\n",
      "        [[0.7614]],\n",
      "\n",
      "        [[0.7023]],\n",
      "\n",
      "        [[0.6950]],\n",
      "\n",
      "        [[0.6859]],\n",
      "\n",
      "        [[0.5325]],\n",
      "\n",
      "        [[0.5652]],\n",
      "\n",
      "        [[0.5069]],\n",
      "\n",
      "        [[0.5687]],\n",
      "\n",
      "        [[0.6665]],\n",
      "\n",
      "        [[0.7632]],\n",
      "\n",
      "        [[0.6213]],\n",
      "\n",
      "        [[0.6073]],\n",
      "\n",
      "        [[0.6004]],\n",
      "\n",
      "        [[0.5833]],\n",
      "\n",
      "        [[0.6978]],\n",
      "\n",
      "        [[0.7670]],\n",
      "\n",
      "        [[0.7306]],\n",
      "\n",
      "        [[0.7249]],\n",
      "\n",
      "        [[0.7415]],\n",
      "\n",
      "        [[0.7172]],\n",
      "\n",
      "        [[0.6502]],\n",
      "\n",
      "        [[0.6239]],\n",
      "\n",
      "        [[0.5695]],\n",
      "\n",
      "        [[0.5800]],\n",
      "\n",
      "        [[0.6485]],\n",
      "\n",
      "        [[0.6517]]])]\n",
      "[tensor([[[[1474., 1694., 1734.,  ..., 1074., 1078., 1064.],\n",
      "          [1732., 1640., 1396.,  ..., 1100., 1096., 1098.],\n",
      "          [1126.,  954.,  866.,  ..., 1120., 1114., 1104.],\n",
      "          ...,\n",
      "          [1396., 1476., 1532.,  ..., 1036., 1004.,  956.],\n",
      "          [1408., 1440., 1508.,  ..., 1180., 1134., 1058.],\n",
      "          [1358., 1388., 1436.,  ..., 1512., 1328., 1190.]],\n",
      "\n",
      "         [[2066., 2216., 2196.,  ..., 1714., 1698., 1676.],\n",
      "          [2020., 1940., 1772.,  ..., 1738., 1716., 1664.],\n",
      "          [1728., 1682., 1668.,  ..., 1738., 1738., 1686.],\n",
      "          ...,\n",
      "          [1832., 1860., 1880.,  ..., 1652., 1676., 1710.],\n",
      "          [1822., 1836., 1868.,  ..., 1662., 1760., 1800.],\n",
      "          [1842., 1854., 1832.,  ..., 1934., 1816., 1746.]],\n",
      "\n",
      "         [[2106., 2266., 2308.,  ..., 1798., 1796., 1776.],\n",
      "          [2132., 2038., 1904.,  ..., 1786., 1792., 1774.],\n",
      "          [1768., 1726., 1702.,  ..., 1804., 1830., 1804.],\n",
      "          ...,\n",
      "          [1916., 1962., 1936.,  ..., 1762., 1796., 1762.],\n",
      "          [1898., 1946., 1944.,  ..., 1856., 1826., 1828.],\n",
      "          [1890., 1870., 1896.,  ..., 2056., 1900., 1828.]]],\n",
      "\n",
      "\n",
      "        [[[1400., 1348., 1304.,  ..., 1790., 1390., 1334.],\n",
      "          [1438., 1320., 1262.,  ..., 1712., 1490., 1454.],\n",
      "          [1314., 1280., 1310.,  ..., 1814., 1382., 1314.],\n",
      "          ...,\n",
      "          [1660., 1602., 1556.,  ..., 1478., 1298., 1310.],\n",
      "          [1528., 1538., 1524.,  ..., 1580., 1488., 1608.],\n",
      "          [1472., 1540., 1530.,  ..., 1504., 1750., 1858.]],\n",
      "\n",
      "         [[1808., 1782., 1776.,  ..., 2078., 1886., 1874.],\n",
      "          [1842., 1812., 1782.,  ..., 2002., 1876., 1976.],\n",
      "          [1784., 1816., 1820.,  ..., 2042., 1896., 1904.],\n",
      "          ...,\n",
      "          [2028., 2064., 2038.,  ..., 1872., 1782., 1770.],\n",
      "          [1996., 1998., 1990.,  ..., 1934., 1846., 1918.],\n",
      "          [2006., 2004., 1958.,  ..., 1896., 2050., 2052.]],\n",
      "\n",
      "         [[1880., 1844., 1854.,  ..., 2136., 1990., 1950.],\n",
      "          [1888., 1850., 1838.,  ..., 2040., 2014., 2024.],\n",
      "          [1866., 1858., 1848.,  ..., 2116., 1994., 1912.],\n",
      "          ...,\n",
      "          [2076., 2074., 2066.,  ..., 2064., 1920., 1932.],\n",
      "          [1998., 2018., 2026.,  ..., 2070., 1998., 2072.],\n",
      "          [1998., 2026., 2010.,  ..., 2044., 2204., 2226.]]],\n",
      "\n",
      "\n",
      "        [[[1490., 1576., 1622.,  ..., 1598., 2126., 2360.],\n",
      "          [1550., 1682., 1688.,  ..., 2098., 2386., 2300.],\n",
      "          [1732., 1706., 1624.,  ..., 1948., 2038., 2090.],\n",
      "          ...,\n",
      "          [1190., 1108., 1120.,  ...,  762.,  768.,  766.],\n",
      "          [1126., 1108., 1144.,  ...,  748.,  768.,  742.],\n",
      "          [1192., 1266., 1402.,  ...,  780.,  740.,  738.]],\n",
      "\n",
      "         [[2072., 2110., 2040.,  ..., 1940., 2320., 2368.],\n",
      "          [2046., 2138., 2080.,  ..., 2252., 2530., 2400.],\n",
      "          [2106., 2106., 2030.,  ..., 2268., 2316., 2240.],\n",
      "          ...,\n",
      "          [1604., 1628., 1644.,  ..., 1418., 1388., 1364.],\n",
      "          [1628., 1656., 1662.,  ..., 1370., 1350., 1280.],\n",
      "          [1694., 1752., 1766.,  ..., 1406., 1350., 1352.]],\n",
      "\n",
      "         [[2044., 2058., 2062.,  ..., 2074., 2436., 2538.],\n",
      "          [2058., 2118., 2098.,  ..., 2414., 2688., 2612.],\n",
      "          [2082., 2088., 2060.,  ..., 2422., 2466., 2376.],\n",
      "          ...,\n",
      "          [1806., 1804., 1776.,  ..., 1638., 1632., 1636.],\n",
      "          [1786., 1792., 1778.,  ..., 1630., 1614., 1602.],\n",
      "          [1832., 1888., 1920.,  ..., 1622., 1606., 1616.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[1372., 1354., 1334.,  ..., 1426., 1442., 1474.],\n",
      "          [1434., 1360., 1348.,  ..., 1456., 1418., 1454.],\n",
      "          [1366., 1330., 1388.,  ..., 1462., 1484., 1506.],\n",
      "          ...,\n",
      "          [1408., 1482., 1570.,  ...,  992.,  972.,  966.],\n",
      "          [1434., 1456., 1490.,  ...,  972.,  946.,  964.],\n",
      "          [1472., 1520., 1550.,  ...,  936.,  982.,  966.]],\n",
      "\n",
      "         [[1812., 1764., 1762.,  ..., 1776., 1786., 1834.],\n",
      "          [1842., 1816., 1802.,  ..., 1830., 1834., 1848.],\n",
      "          [1824., 1824., 1822.,  ..., 1854., 1822., 1814.],\n",
      "          ...,\n",
      "          [1942., 1970., 1988.,  ..., 1602., 1590., 1592.],\n",
      "          [1982., 1964., 1966.,  ..., 1642., 1618., 1592.],\n",
      "          [1980., 1986., 2006.,  ..., 1616., 1626., 1604.]],\n",
      "\n",
      "         [[1956., 1924., 1946.,  ..., 1924., 1960., 2004.],\n",
      "          [1984., 1944., 1942.,  ..., 1954., 1956., 1986.],\n",
      "          [1978., 1946., 1924.,  ..., 1976., 1966., 1972.],\n",
      "          ...,\n",
      "          [1936., 1936., 1992.,  ..., 1740., 1738., 1754.],\n",
      "          [1956., 1934., 1930.,  ..., 1736., 1734., 1768.],\n",
      "          [1962., 1980., 1962.,  ..., 1734., 1746., 1742.]]],\n",
      "\n",
      "\n",
      "        [[[1662., 1620., 1656.,  ...,  948.,  946.,  954.],\n",
      "          [1728., 1696., 1718.,  ...,  970.,  964.,  970.],\n",
      "          [1668., 1666., 1686.,  ..., 1008., 1008., 1050.],\n",
      "          ...,\n",
      "          [1432., 1474., 1552.,  ..., 1132., 1212., 1206.],\n",
      "          [1516., 1624., 1630.,  ..., 1222., 1212., 1190.],\n",
      "          [1516., 1602., 1578.,  ..., 1154., 1160., 1156.]],\n",
      "\n",
      "         [[2026., 2028., 2060.,  ..., 1590., 1594., 1584.],\n",
      "          [2060., 2040., 2044.,  ..., 1560., 1574., 1568.],\n",
      "          [2034., 2030., 2040.,  ..., 1610., 1654., 1730.],\n",
      "          ...,\n",
      "          [1988., 1974., 1988.,  ..., 1732., 1742., 1724.],\n",
      "          [1994., 1980., 1954.,  ..., 1770., 1746., 1718.],\n",
      "          [1924., 1952., 1940.,  ..., 1748., 1736., 1774.]],\n",
      "\n",
      "         [[2034., 2032., 2026.,  ..., 1750., 1732., 1726.],\n",
      "          [2048., 2034., 2048.,  ..., 1734., 1726., 1728.],\n",
      "          [2052., 2050., 2012.,  ..., 1744., 1768., 1808.],\n",
      "          ...,\n",
      "          [1898., 1946., 1956.,  ..., 1842., 1840., 1872.],\n",
      "          [1962., 2018., 2012.,  ..., 1904., 1844., 1832.],\n",
      "          [1936., 1984., 1982.,  ..., 1856., 1856., 1872.]]],\n",
      "\n",
      "\n",
      "        [[[1534., 1560., 1458.,  ...,  966.,  998., 1022.],\n",
      "          [1586., 1472., 1348.,  ...,  904.,  912.,  906.],\n",
      "          [1570., 1402., 1262.,  ...,  912.,  910.,  912.],\n",
      "          ...,\n",
      "          [1188., 1208., 1256.,  ..., 1322., 1342., 1354.],\n",
      "          [1374., 1256., 1222.,  ..., 1328., 1344., 1378.],\n",
      "          [1422., 1312., 1240.,  ..., 1310., 1348., 1344.]],\n",
      "\n",
      "         [[1924., 1934., 1912.,  ..., 1664., 1702., 1714.],\n",
      "          [2022., 1924., 1838.,  ..., 1658., 1618., 1600.],\n",
      "          [1978., 1858., 1830.,  ..., 1602., 1596., 1594.],\n",
      "          ...,\n",
      "          [1930., 1910., 1886.,  ..., 1818., 1830., 1838.],\n",
      "          [1916., 1906., 1898.,  ..., 1792., 1816., 1850.],\n",
      "          [1888., 1900., 1918.,  ..., 1792., 1796., 1802.]],\n",
      "\n",
      "         [[1932., 1952., 1916.,  ..., 1764., 1786., 1788.],\n",
      "          [1996., 1944., 1870.,  ..., 1714., 1716., 1726.],\n",
      "          [1974., 1914., 1848.,  ..., 1710., 1728., 1732.],\n",
      "          ...,\n",
      "          [1892., 1894., 1882.,  ..., 1874., 1854., 1870.],\n",
      "          [1944., 1920., 1892.,  ..., 1870., 1874., 1896.],\n",
      "          [2022., 1942., 1892.,  ..., 1838., 1858., 1900.]]]]), tensor([[[0.6499]],\n",
      "\n",
      "        [[0.6627]],\n",
      "\n",
      "        [[0.6758]],\n",
      "\n",
      "        [[0.6949]],\n",
      "\n",
      "        [[0.6490]],\n",
      "\n",
      "        [[0.7926]],\n",
      "\n",
      "        [[0.7107]],\n",
      "\n",
      "        [[0.7972]],\n",
      "\n",
      "        [[0.7766]],\n",
      "\n",
      "        [[0.5763]],\n",
      "\n",
      "        [[0.6508]],\n",
      "\n",
      "        [[0.6439]],\n",
      "\n",
      "        [[0.6031]],\n",
      "\n",
      "        [[0.7036]],\n",
      "\n",
      "        [[0.6406]],\n",
      "\n",
      "        [[0.6514]],\n",
      "\n",
      "        [[0.6366]],\n",
      "\n",
      "        [[0.7404]],\n",
      "\n",
      "        [[0.5817]],\n",
      "\n",
      "        [[0.5514]],\n",
      "\n",
      "        [[0.7043]],\n",
      "\n",
      "        [[0.7872]],\n",
      "\n",
      "        [[0.6525]],\n",
      "\n",
      "        [[0.6102]],\n",
      "\n",
      "        [[0.6223]],\n",
      "\n",
      "        [[0.6874]],\n",
      "\n",
      "        [[0.5506]],\n",
      "\n",
      "        [[0.4533]],\n",
      "\n",
      "        [[0.4544]],\n",
      "\n",
      "        [[0.5542]],\n",
      "\n",
      "        [[0.6212]],\n",
      "\n",
      "        [[0.7423]],\n",
      "\n",
      "        [[0.6079]],\n",
      "\n",
      "        [[0.6580]],\n",
      "\n",
      "        [[0.6491]],\n",
      "\n",
      "        [[0.6571]],\n",
      "\n",
      "        [[0.7336]],\n",
      "\n",
      "        [[0.7703]],\n",
      "\n",
      "        [[0.7677]],\n",
      "\n",
      "        [[0.7245]],\n",
      "\n",
      "        [[0.7634]],\n",
      "\n",
      "        [[0.6861]],\n",
      "\n",
      "        [[0.7614]],\n",
      "\n",
      "        [[0.7567]],\n",
      "\n",
      "        [[0.7515]],\n",
      "\n",
      "        [[0.6354]],\n",
      "\n",
      "        [[0.6320]],\n",
      "\n",
      "        [[0.6882]],\n",
      "\n",
      "        [[0.5374]],\n",
      "\n",
      "        [[0.5585]],\n",
      "\n",
      "        [[0.6533]],\n",
      "\n",
      "        [[0.6674]],\n",
      "\n",
      "        [[0.6609]],\n",
      "\n",
      "        [[0.7702]],\n",
      "\n",
      "        [[0.6773]],\n",
      "\n",
      "        [[0.7033]],\n",
      "\n",
      "        [[0.7431]],\n",
      "\n",
      "        [[0.6636]],\n",
      "\n",
      "        [[0.6215]],\n",
      "\n",
      "        [[0.6252]],\n",
      "\n",
      "        [[0.6181]],\n",
      "\n",
      "        [[0.6198]],\n",
      "\n",
      "        [[0.6486]],\n",
      "\n",
      "        [[0.6577]]])]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_549931/2477719503.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# scalars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# scalars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for d in dataloader:\n",
    "    print (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee670be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
